{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile #pip install Pillow==9.4.0\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import glob\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from ImageCrop import ImagePreprocessor\n",
    "from SpotterWrapper import Spotter, PolygonVisualizer\n",
    "from IPython.display import display\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Specify filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name folders for raw data and processed data\n",
    "map_data_topfolder = 'raw_maps_20231024'\n",
    "map_strec_topfolder = 'processed/strec'\n",
    "\n",
    "for fp in [map_strec_topfolder]:\n",
    "    if not os.path.isdir(fp):\n",
    "        os.makedirs(fp)\n",
    "\n",
    "# IMPORTANT! Locate spotter directory and detectron weights\n",
    "git_clone_location = 'C:/repo/'\n",
    "spotter_directory = git_clone_location + 'mapkurator-spotter/spotter-v2'\n",
    "model_weights = git_clone_location + 'detectron2-master/detectron2/checkpoint/model_v2_en.pth'\n",
    "spotter_config = spotter_directory + '/configs/PALEJUN/Finetune/Rumsey_Polygon_Finetune.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Crop all jpeg maps in (user defined) map_data_topfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_scan(img_path, output_dir, save_each_layer=False):\n",
    "    image = Image.open(img_path)\n",
    "    image_preprocessor = ImagePreprocessor(image, overlapping_tolerance=0.3, num_layers=5, min_patch_resolution=512, max_patch_resolution=4096)\n",
    "    image_preprocessor.process()\n",
    "    print(\"preprocessing done\")\n",
    "    spotter = Spotter(spotter_config, model_weights, confidence_thresh=0.8, draw_thresh=0.85)\n",
    "    all_layer_results = []\n",
    "\n",
    "    base_image_batch, base_offset_xs, base_offset_ys = image_preprocessor.get_image_patches(0)\n",
    "    vis = PolygonVisualizer()\n",
    "    vis.canvas_from_patches(base_image_batch, base_offset_xs, base_offset_ys)\n",
    "\n",
    "    for i in range(image_preprocessor.num_layers):\n",
    "        # If you want to save for each layer, uncomment the following line\n",
    "        # image_preprocessor.save_patches(os.path.join(output_dir, f'layer_{i}_patches'), layer=i)\n",
    "\n",
    "        image_batch, offset_xs, offset_ys = image_preprocessor.get_image_patches(i)\n",
    "        spotter.load_batch(image_batch, offset_xs, offset_ys)\n",
    "        results = spotter.inference_batch()\n",
    "        all_layer_results.extend(results)\n",
    "\n",
    "        #all_layer_offset_xs.extend(offset_xs)\n",
    "        #all_layer_offset_ys.extend(offset_ys)\n",
    "\n",
    "        if save_each_layer == True:\n",
    "            vis.draw(results).save(os.path.join(output_dir, f'combined_tagged_{i}.png'))\n",
    "            vis.save_json(results, os.path.join(output_dir, f'combined_tagged_{i}.json'))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    vis.draw(all_layer_results).save(os.path.join(output_dir, f'combined_tagged_all_layers.png'))\n",
    "    vis.save_json(all_layer_results, os.path.join(output_dir, f'combined_tagged_all_layers.json'))\n",
    "\n",
    "# Run crop on all maps\n",
    "for map_data_subfolder in next(os.walk(map_data_topfolder))[1]:\n",
    "    jpeg_list = glob.glob(map_data_topfolder + '/' + map_data_subfolder + '/*.jpeg')\n",
    "    if len(jpeg_list) != 1:\n",
    "        print(map_data_subfolder + \" failed. Please ensure there is exactly 1 file with extension .jpeg in the folder.\")\n",
    "    else:\n",
    "        map_image = jpeg_list[0].split(\"\\\\\")[1]\n",
    "        if map_data_subfolder in ['1846_vandevelde', '1874_saunders', '1845_kiepert']: # '1858_vandevelde', '1874_saunders', '1845_kiepert']: #,,]: #'1858_vandevelde', '1847_tobler', '1845_kiepert'\n",
    "            img_path = map_data_topfolder + '/' + map_data_subfolder + \"/\" + map_image\n",
    "            map_name = os.path.basename(img_path).split('.')[0] # get the map name without extension\n",
    "            output_dir = os.path.join(map_strec_topfolder, map_name)\n",
    "            if not os.path.isdir(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            pyramid_scan(img_path, output_dir, save_each_layer=False)\n",
    "            logging.info('Done cropping %s' %img_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Label Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import json \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from itertools import combinations\n",
    "import scipy\n",
    "import numpy as np\n",
    "import importlib\n",
    "import Clustering\n",
    "import TextRectify\n",
    "import TextAmalgamate\n",
    "import ExtractHandling\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(TextRectify)\n",
    "importlib.reload(TextAmalgamate)\n",
    "importlib.reload(ExtractHandling)\n",
    "\n",
    "map_name_in_strec = 'saunders_1874'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Text Rectification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_cluster_pre_merge = True\n",
    "\n",
    "with open(f'processed/strec/{map_name_in_strec}/combined_tagged_all_layers.json', 'r', encoding='utf-8') as f:\n",
    "\n",
    "    clustered = Clustering.cluster_polygons(json.load(f))\n",
    "\n",
    "    # visualize clusters\n",
    "    #image = Clustering.visualize_polygons(clustered, 'processed/strec/kiepert_1845/raw.jpeg')\n",
    "    #image.save('processed/strec/kiepert_1845/combined_tagged_all_layers_clustering.png')\n",
    "\n",
    "for label, cluster in clustered.items():\n",
    "    texts = []\n",
    "    scores = []\n",
    "    for polygon in cluster:\n",
    "        texts.append(polygon['text'])\n",
    "        scores.append(polygon['score'])\n",
    "\n",
    "    rectifier = TextRectify.TextRectifier(0.95, 0.5, 10, True, True)\n",
    "\n",
    "    rectifier.feed_data(texts, scores)\n",
    "\n",
    "    rectifier.fit()\n",
    "\n",
    "    rectified, mask = rectifier.get_rectified_text()\n",
    "\n",
    "    if rectified is None:\n",
    "        rectified = max(texts, key=len)\n",
    "\n",
    "    for i in range(len(cluster)):\n",
    "        cluster[i]['text'] = rectified[i]\n",
    "        cluster[i]['keep'] = mask[i]\n",
    "\n",
    "image = Clustering.visualize_polygons(clustered, f'processed/strec/{map_name_in_strec}/raw.jpeg')\n",
    "image.save(f'processed/strec/{map_name_in_strec}/combined_tagged_all_layers_rectified.png')\n",
    "\n",
    "polygon_x = {}\n",
    "polygon_y = {}\n",
    "texts = {}\n",
    "scores = {}\n",
    "i = 0\n",
    "for label, cluster in clustered.items():\n",
    "    for polygon in cluster:\n",
    "        if do_cluster_pre_merge:\n",
    "            if polygon['keep']:\n",
    "                polygon_x[str(i)] = polygon['polygon_x']\n",
    "                polygon_y[str(i)] = polygon['polygon_y']\n",
    "                texts[str(i)] = polygon['text']\n",
    "                scores[str(i)] = polygon['score']\n",
    "                i += 1\n",
    "        else:\n",
    "            polygon_x[str(i)] = polygon['polygon_x']\n",
    "            polygon_y[str(i)] = polygon['polygon_y']\n",
    "            texts[str(i)] = polygon['text']\n",
    "            scores[str(i)] = polygon['score']\n",
    "            i += 1\n",
    "\n",
    "json_data = {'polygon_x': polygon_x, 'polygon_y': polygon_y, 'text': texts, 'score': scores}\n",
    "\n",
    "with open(f'processed/strec/{map_name_in_strec}/combined_tagged_all_layers_rectified_premerge.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Text Amalgamation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amalgamation stage - assumes there exists \"combined_tagged_all_layers_rectified_premerge.json\" in map_name_in_strec processed folder.\n",
    "df = ExtractHandling.prepare_labels_for_amalgamation(map_name_in_strec)\n",
    "df = TextAmalgamate.amalgamate_labels_wrapper(df, 0.75, .5)\n",
    "\n",
    "# Save amalgamated labels\n",
    "with open(f'processed/strec/{map_name_in_strec}/amalgamate.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list(df[\"labels\"])\n",
    "polygons = []\n",
    "texts = []\n",
    "PCA_features = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    poly = result[i][0]\n",
    "    polygons.append(poly)\n",
    "    texts.append(result[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA features calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\26553\\anaconda3\\envs\\mapKurator\\lib\\site-packages\\PIL\\Image.py:3035: DecompressionBombWarning: Image size (172140000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\26553\\AppData\\Local\\Temp\\ipykernel_5668\\3317146773.py:17: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  vis.draw_poly(polygons, texts, PCA_features)\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\SpotterWrapper.py:237: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  for p in poly:\n"
     ]
    }
   ],
   "source": [
    "#reload SpotterWrapper module\n",
    "import importlib\n",
    "import SpotterWrapper\n",
    "import Grouping\n",
    "\n",
    "importlib.reload(SpotterWrapper)\n",
    "importlib.reload(Grouping)\n",
    "\n",
    "PCA_features = Grouping.calc_PCA_feats(polygons, do_separation=True, enhance_coords=True)\n",
    "\n",
    "print(\"PCA features calculated.\")\n",
    "\n",
    "vis = SpotterWrapper.PolygonVisualizer()\n",
    "canvas = Image.open(f'processed/strec/{map_name_in_strec}/raw.jpeg')\n",
    "vis.canvas_from_image(canvas)\n",
    "\n",
    "vis.draw_poly(polygons, texts, PCA_features)\n",
    "\n",
    "vis.save(f'processed/strec/{map_name_in_strec}/output.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: Isolate crops to be used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "def visualize_crop(map_name_in_strec, raw_or_spotter, left_x, right_x, top_y, bottom_y):\n",
    "    if raw_or_spotter == \"raw\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/raw.jpeg') \n",
    "    elif raw_or_spotter == \"spotter_0\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_0.png')\n",
    "    elif raw_or_spotter == \"spotter_1\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_1.png')\n",
    "    elif raw_or_spotter == \"spotter_2\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_2.png')\n",
    "    elif raw_or_spotter == \"all\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_all_layers.png')\n",
    "    elif raw_or_spotter == \"rectified\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_all_layers_rectified.png')\n",
    "    width, height = map_img.size\n",
    "    print(\"full map is \" + str(width) + \" pixels wide by \" + str(height) + \" pixels high.\\n displaying crop:\")\n",
    "    display(map_img.crop((left_x, top_y, right_x, bottom_y, )))\n",
    "\n",
    "left_x = 2475\n",
    "right_x = 3550\n",
    "top_y = 4820\n",
    "bottom_y = 5850\n",
    "\n",
    "#visualize_crop(\"kiepert_1845\", \"all\", left_x, right_x, top_y, bottom_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiepert_gt_patch_1 = [2475, 3550, 4820, 5850]\n",
    "saunders_gt_patch_1 = [3150, 4150, 2250, 3250]\n",
    "saunders_gt_patch_2 = [6750, 7750, 2250, 3250]\n",
    "saunders_gt_patch_3 = [5400, 6400, 4500, 5500]\n",
    "saunders_gt_patch_4 = [7650, 8650, 5400, 6400]\n",
    "saunders_gt_patch_5 = [7650, 8650, 3150, 4150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Precision and Recall: IoU after 1:1 Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "## Full pipeline \n",
    "num_detected_kiepert, num_gt_kiepert, IoU_pairs_kiepert = geographic_evaluation(\"kiepert_1845\", \"components\", [2475, 3550, 4820, 5850])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_detected, num_gt, IoU_pairs = geographic_evaluation(\"kiepert_1845\", \"components\")\n",
    "IoU_pairs = pd.DataFrame(IoU_pairs, columns=['geo_IoU', 'spotter_txt', 'gt_txt'])\n",
    "\n",
    "print(\"Avg of Geographic Precision: \" + str(IoU_pairs['geo_IoU'].astype(float).sum(axis=0) / num_detected))\n",
    "print(\"Avg of Geographic Recall: \" + str(IoU_pairs['geo_IoU'].astype(float).sum(axis=0) / num_gt))\n",
    "\n",
    "IoU_pairs['normalized_txt_similarity'] = IoU_pairs.apply(lambda row: text_compare(row['spotter_txt'], row['gt_txt']), axis=1)\n",
    "\n",
    "print(\"Avg of Text Precision: \" + str(IoU_pairs['normalized_txt_similarity'].astype(float).sum(axis=0) / num_detected))\n",
    "print(\"Avg of Text Recall: \" + str(IoU_pairs['normalized_txt_similarity'].astype(float).sum(axis=0) / num_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_detected, num_gt, IoU_pairs = geographic_evaluation(\"kiepert_1845\", \"components\", \"combined_tagged_0.json\")\n",
    "IoU_pairs = pd.DataFrame(IoU_pairs, columns=['geo_IoU', 'spotter_txt', 'gt_txt'])\n",
    "\n",
    "print(\"Avg of Geographic Precision: \" + str(IoU_pairs['geo_IoU'].astype(float).sum(axis=0) / num_detected))\n",
    "print(\"Avg of Geographic Recall: \" + str(IoU_pairs['geo_IoU'].astype(float).sum(axis=0) / num_gt))\n",
    "\n",
    "IoU_pairs['normalized_txt_similarity'] = IoU_pairs.apply(lambda row: text_compare(row['spotter_txt'], row['gt_txt']), axis=1)\n",
    "\n",
    "print(\"Avg of Text Precision: \" + str(IoU_pairs['normalized_txt_similarity'].astype(float).sum(axis=0) / num_detected))\n",
    "print(\"Avg of Text Recall: \" + str(IoU_pairs['normalized_txt_similarity'].astype(float).sum(axis=0) / num_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate versions\n",
    "\n",
    "num_detected_kiepert, num_gt_kiepert, IoU_pairs_kiepert = geographic_evaluation(\"kiepert_1845\", \"components\", [2475, 3550, 4820, 5850])\n",
    "num_detected_saunders1, num_gt_saunders1, IoU_pairs_saunders1 = geographic_evaluation(\"saunders_1874\", \"components\", [3150, 4150, 2250, 3250])\n",
    "num_detected_saunders2, num_gt_saunders2, IoU_pairs_saunders2 = geographic_evaluation(\"saunders_1874\", \"components\", [6750, 7750, 2250, 3250])\n",
    "num_detected_saunders3, num_gt_saunders3, IoU_pairs_saunders3 = geographic_evaluation(\"saunders_1874\", \"components\", [5400, 6400, 4500, 5500])\n",
    "num_detected_saunders4, num_gt_saunders4, IoU_pairs_saunders4 = geographic_evaluation(\"saunders_1874\", \"components\", [7650, 8650, 5400, 6400])\n",
    "num_detected_saunders5, num_gt_saunders5, IoU_pairs_saunders5 = geographic_evaluation(\"saunders_1874\", \"components\", [7650, 8650, 3150, 4150])\n",
    "\n",
    "num_gt_kiepert = num_gt_kiepert\n",
    "num_gt_saunders = num_gt_saunders1 + num_gt_saunders2 + num_gt_saunders3 + num_gt_saunders4 + num_gt_saunders5\n",
    "num_detected_pyramid_kiepert = num_detected_kiepert\n",
    "num_detected_pyramid_saunders = num_detected_saunders1 + num_detected_saunders2 + num_detected_saunders3 + num_detected_saunders4 + num_detected_saunders5\n",
    "IoU_pairs_pyramid_kiepert = IoU_pairs_kiepert\n",
    "IoU_pairs_pyramid_saunders = np.concatenate((IoU_pairs_saunders1, IoU_pairs_saunders2, IoU_pairs_saunders3, IoU_pairs_saunders4, IoU_pairs_saunders5))\n",
    "\n",
    "num_detected_kiepert, num_gt_kiepert, IoU_pairs_kiepert = geographic_evaluation(\"kiepert_1845\", \"components\", [2475, 3550, 4820, 5850], \"combined_tagged_0.json\")\n",
    "num_detected_saunders1, num_gt_saunders1, IoU_pairs_saunders1 = geographic_evaluation(\"saunders_1874\", \"components\", [3150, 4150, 2250, 3250], \"combined_tagged_0.json\")\n",
    "num_detected_saunders2, num_gt_saunders2, IoU_pairs_saunders2 = geographic_evaluation(\"saunders_1874\", \"components\", [6750, 7750, 2250, 3250], \"combined_tagged_0.json\")\n",
    "num_detected_saunders3, num_gt_saunders3, IoU_pairs_saunders3 = geographic_evaluation(\"saunders_1874\", \"components\", [5400, 6400, 4500, 5500], \"combined_tagged_0.json\")\n",
    "num_detected_saunders4, num_gt_saunders4, IoU_pairs_saunders4 = geographic_evaluation(\"saunders_1874\", \"components\", [7650, 8650, 5400, 6400], \"combined_tagged_0.json\")\n",
    "num_detected_saunders5, num_gt_saunders5, IoU_pairs_saunders5 = geographic_evaluation(\"saunders_1874\", \"components\", [7650, 8650, 3150, 4150], \"combined_tagged_0.json\")\n",
    "\n",
    "num_detected_flat_kiepert = num_detected_kiepert\n",
    "num_detected_flat_saunders = num_detected_saunders1 + num_detected_saunders2 + num_detected_saunders3 + num_detected_saunders4 + num_detected_saunders5\n",
    "IoU_pairs_flat_saunders = np.concatenate((IoU_pairs_saunders1, IoU_pairs_saunders2, IoU_pairs_saunders3, IoU_pairs_saunders4, IoU_pairs_saunders5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec_rec(IoU_pairs, num_detected, num_gt):\n",
    "    print(\"Avg of Geographic Precision: \" + str(IoU_pairs['geo_IoU'].astype(float).sum(axis=0) / num_detected))\n",
    "    print(\"Avg of Geographic Recall: \" + str(IoU_pairs['geo_IoU'].astype(float).sum(axis=0) / num_gt))\n",
    "    IoU_pairs['normalized_txt_similarity'] = IoU_pairs.apply(lambda row: text_compare(row['spotter_txt'], row['gt_txt']), axis=1)\n",
    "    print(\"Avg of Text Precision: \" + str(IoU_pairs['normalized_txt_similarity'].astype(float).sum(axis=0) / num_detected))\n",
    "    print(\"Avg of Text Recall: \" + str(IoU_pairs['normalized_txt_similarity'].astype(float).sum(axis=0) / num_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IoU_pairs = pd.DataFrame(IoU_pairs_pyramid_kiepert, columns=['geo_IoU', 'spotter_txt', 'gt_txt'])\n",
    "prec_rec(IoU_pairs, num_detected_pyramid_kiepert, num_gt_kiepert)\n",
    "IoU_pairs = pd.DataFrame(IoU_pairs_pyramid_saunders, columns=['geo_IoU', 'spotter_txt', 'gt_txt'])\n",
    "prec_rec(IoU_pairs, num_detected_pyramid_saunders, num_gt_saunders)\n",
    "IoU_pairs = pd.DataFrame(IoU_pairs_flat_saunders, columns=['geo_IoU', 'spotter_txt', 'gt_txt'])\n",
    "prec_rec(IoU_pairs, num_detected_flat_saunders, num_gt_saunders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
