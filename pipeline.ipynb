{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile #pip install Pillow==9.4.0\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import glob\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from ImageCrop import ImagePreprocessor\n",
    "from SpotterWrapper import Spotter, PolygonVisualizer\n",
    "from IPython.display import display\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Specify filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name folders for raw data and processed data\n",
    "map_data_topfolder = 'raw_maps_20231024'\n",
    "map_strec_topfolder = 'processed/strec'\n",
    "\n",
    "for fp in [map_strec_topfolder]:\n",
    "    if not os.path.isdir(fp):\n",
    "        os.makedirs(fp)\n",
    "\n",
    "# IMPORTANT! Locate spotter directory and detectron weights\n",
    "git_clone_location = 'C:/repo/'\n",
    "spotter_directory = git_clone_location + 'mapkurator-spotter/spotter-v2'\n",
    "model_weights = git_clone_location + 'detectron2-master/detectron2/checkpoint/model_v2_en.pth'\n",
    "spotter_config = spotter_directory + '/configs/PALEJUN/Finetune/Rumsey_Polygon_Finetune.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Crop all jpeg maps in (user defined) map_data_topfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_scan(img_path, output_dir, save_each_layer=False):\n",
    "    image = Image.open(img_path)\n",
    "    image_preprocessor = ImagePreprocessor(image, overlapping_tolerance=0.3, num_layers=5, min_patch_resolution=512, max_patch_resolution=4096)\n",
    "    image_preprocessor.process()\n",
    "    print(\"preprocessing done\")\n",
    "    spotter = Spotter(spotter_config, model_weights, confidence_thresh=0.8, draw_thresh=0.85)\n",
    "    all_layer_results = []\n",
    "\n",
    "    base_image_batch, base_offset_xs, base_offset_ys = image_preprocessor.get_image_patches(0)\n",
    "    vis = PolygonVisualizer()\n",
    "    vis.canvas_from_patches(base_image_batch, base_offset_xs, base_offset_ys)\n",
    "\n",
    "    for i in range(image_preprocessor.num_layers):\n",
    "        # If you want to save for each layer, uncomment the following line\n",
    "        # image_preprocessor.save_patches(os.path.join(output_dir, f'layer_{i}_patches'), layer=i)\n",
    "\n",
    "        image_batch, offset_xs, offset_ys = image_preprocessor.get_image_patches(i)\n",
    "        spotter.load_batch(image_batch, offset_xs, offset_ys)\n",
    "        results = spotter.inference_batch()\n",
    "        all_layer_results.extend(results)\n",
    "\n",
    "        #all_layer_offset_xs.extend(offset_xs)\n",
    "        #all_layer_offset_ys.extend(offset_ys)\n",
    "\n",
    "        if save_each_layer == True:\n",
    "            vis.draw(results).save(os.path.join(output_dir, f'combined_tagged_{i}.png'))\n",
    "            vis.save_json(results, os.path.join(output_dir, f'combined_tagged_{i}.json'))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    vis.draw(all_layer_results).save(os.path.join(output_dir, f'combined_tagged_all_layers.png'))\n",
    "    vis.save_json(all_layer_results, os.path.join(output_dir, f'combined_tagged_all_layers.json'))\n",
    "\n",
    "# Run crop on all maps\n",
    "for map_data_subfolder in next(os.walk(map_data_topfolder))[1]:\n",
    "    jpeg_list = glob.glob(map_data_topfolder + '/' + map_data_subfolder + '/*.jpeg')\n",
    "    if len(jpeg_list) != 1:\n",
    "        print(map_data_subfolder + \" failed. Please ensure there is exactly 1 file with extension .jpeg in the folder.\")\n",
    "    else:\n",
    "        map_image = jpeg_list[0].split(\"\\\\\")[1]\n",
    "        if map_data_subfolder in ['1846_vandevelde', '1874_saunders', '1845_kiepert']: # '1858_vandevelde', '1874_saunders', '1845_kiepert']: #,,]: #'1858_vandevelde', '1847_tobler', '1845_kiepert'\n",
    "            img_path = map_data_topfolder + '/' + map_data_subfolder + \"/\" + map_image\n",
    "            map_name = os.path.basename(img_path).split('.')[0] # get the map name without extension\n",
    "            output_dir = os.path.join(map_strec_topfolder, map_name)\n",
    "            if not os.path.isdir(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            pyramid_scan(img_path, output_dir, save_each_layer=False)\n",
    "            logging.info('Done cropping %s' %img_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Label Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import json \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from itertools import combinations\n",
    "import scipy\n",
    "import numpy as np\n",
    "import importlib\n",
    "import Clustering\n",
    "import TextRectify\n",
    "import TextAmalgamate\n",
    "import ExtractHandling\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(TextRectify)\n",
    "importlib.reload(TextAmalgamate)\n",
    "importlib.reload(ExtractHandling)\n",
    "\n",
    "map_name_in_strec = 'saunders_1874'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Text Rectification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_cluster_pre_merge = True\n",
    "\n",
    "with open(f'processed/strec/{map_name_in_strec}/combined_tagged_all_layers.json', 'r', encoding='utf-8') as f:\n",
    "\n",
    "    clustered = Clustering.cluster_polygons(json.load(f))\n",
    "\n",
    "    # visualize clusters\n",
    "    #image = Clustering.visualize_polygons(clustered, 'processed/strec/kiepert_1845/raw.jpeg')\n",
    "    #image.save('processed/strec/kiepert_1845/combined_tagged_all_layers_clustering.png')\n",
    "\n",
    "for label, cluster in clustered.items():\n",
    "    texts = []\n",
    "    scores = []\n",
    "    for polygon in cluster:\n",
    "        texts.append(polygon['text'])\n",
    "        scores.append(polygon['score'])\n",
    "\n",
    "    rectifier = TextRectify.TextRectifier(0.95, 0.5, 10, True, True)\n",
    "\n",
    "    rectifier.feed_data(texts, scores)\n",
    "\n",
    "    rectifier.fit()\n",
    "\n",
    "    rectified, mask = rectifier.get_rectified_text()\n",
    "\n",
    "    if rectified is None:\n",
    "        rectified = max(texts, key=len)\n",
    "\n",
    "    for i in range(len(cluster)):\n",
    "        cluster[i]['text'] = rectified[i]\n",
    "        cluster[i]['keep'] = mask[i]\n",
    "\n",
    "image = Clustering.visualize_polygons(clustered, f'processed/strec/{map_name_in_strec}/raw.jpeg')\n",
    "image.save(f'processed/strec/{map_name_in_strec}/combined_tagged_all_layers_rectified.png')\n",
    "\n",
    "polygon_x = {}\n",
    "polygon_y = {}\n",
    "texts = {}\n",
    "scores = {}\n",
    "i = 0\n",
    "for label, cluster in clustered.items():\n",
    "    for polygon in cluster:\n",
    "        if do_cluster_pre_merge:\n",
    "            if polygon['keep']:\n",
    "                polygon_x[str(i)] = polygon['polygon_x']\n",
    "                polygon_y[str(i)] = polygon['polygon_y']\n",
    "                texts[str(i)] = polygon['text']\n",
    "                scores[str(i)] = polygon['score']\n",
    "                i += 1\n",
    "        else:\n",
    "            polygon_x[str(i)] = polygon['polygon_x']\n",
    "            polygon_y[str(i)] = polygon['polygon_y']\n",
    "            texts[str(i)] = polygon['text']\n",
    "            scores[str(i)] = polygon['score']\n",
    "            i += 1\n",
    "\n",
    "json_data = {'polygon_x': polygon_x, 'polygon_y': polygon_y, 'text': texts, 'score': scores}\n",
    "\n",
    "with open(f'processed/strec/{map_name_in_strec}/combined_tagged_all_layers_rectified_premerge.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Text Amalgamation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amalgamation stage - assumes there exists \"combined_tagged_all_layers_rectified_premerge.json\" in map_name_in_strec processed folder.\n",
    "df = ExtractHandling.prepare_labels_for_amalgamation(map_name_in_strec)\n",
    "df = TextAmalgamate.amalgamate_labels_wrapper(df, 0.75, .5)\n",
    "\n",
    "# Save amalgamated labels\n",
    "with open(f'processed/strec/{map_name_in_strec}/amalgamate.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list(df[\"labels\"])\n",
    "polygons = []\n",
    "texts = []\n",
    "PCA_features = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    poly = result[i][0]\n",
    "    polygons.append(poly)\n",
    "    texts.append(result[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\Grouping.py:42: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  polygon_y = p.exterior.coords.xy[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA features calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\26553\\anaconda3\\envs\\mapKurator\\lib\\site-packages\\PIL\\Image.py:3035: DecompressionBombWarning: Image size (172140000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\26553\\AppData\\Local\\Temp\\ipykernel_5668\\3317146773.py:17: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  vis.draw_poly(polygons, texts, PCA_features)\n",
      "c:\\repo\\jerusalem_maps_epfl_dh405\\SpotterWrapper.py:237: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  for p in poly:\n"
     ]
    }
   ],
   "source": [
    "#reload SpotterWrapper module\n",
    "import importlib\n",
    "import SpotterWrapper\n",
    "import Grouping\n",
    "\n",
    "importlib.reload(SpotterWrapper)\n",
    "importlib.reload(Grouping)\n",
    "\n",
    "PCA_features = Grouping.calc_PCA_feats(polygons, do_separation=True, enhance_coords=True)\n",
    "\n",
    "print(\"PCA features calculated.\")\n",
    "\n",
    "vis = SpotterWrapper.PolygonVisualizer()\n",
    "canvas = Image.open(f'processed/strec/{map_name_in_strec}/raw.jpeg')\n",
    "vis.canvas_from_image(canvas)\n",
    "\n",
    "vis.draw_poly(polygons, texts, PCA_features)\n",
    "\n",
    "vis.save(f'processed/strec/{map_name_in_strec}/output.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: Isolate crops to be used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "def visualize_crop(map_name_in_strec, raw_or_spotter, left_x, right_x, top_y, bottom_y):\n",
    "    if raw_or_spotter == \"raw\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/raw.jpeg') \n",
    "    elif raw_or_spotter == \"spotter_0\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_0.png')\n",
    "    elif raw_or_spotter == \"spotter_1\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_1.png')\n",
    "    elif raw_or_spotter == \"spotter_2\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_2.png')\n",
    "    elif raw_or_spotter == \"all\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_all_layers.png')\n",
    "    elif raw_or_spotter == \"rectified\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_all_layers_rectified.png')\n",
    "    width, height = map_img.size\n",
    "    print(\"full map is \" + str(width) + \" pixels wide by \" + str(height) + \" pixels high.\\n displaying crop:\")\n",
    "    display(map_img.crop((left_x, top_y, right_x, bottom_y, )))\n",
    "\n",
    "left_x = 2475\n",
    "right_x = 3550\n",
    "top_y = 4820\n",
    "bottom_y = 5850\n",
    "\n",
    "#visualize_crop(\"kiepert_1845\", \"all\", left_x, right_x, top_y, bottom_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiepert_gt_patch_1 = [2475, 3550, 4820, 5850]\n",
    "saunders_gt_patch_1 = [3150, 4150, 2250, 3250]\n",
    "saunders_gt_patch_2 = [6750, 7750, 2250, 3250]\n",
    "saunders_gt_patch_3 = [5400, 6400, 4500, 5500]\n",
    "saunders_gt_patch_4 = [7650, 8650, 5400, 6400]\n",
    "saunders_gt_patch_5 = [7650, 8650, 3150, 4150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Precision and Recall: IoU after 1:1 Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "## Load in (1) ground truth labels that were built using Via and (2) spotter labels\n",
    "def load_ground_truth_labels(map_name_in_strec, multiline_handling, labels_on_fullsize_map=True):\n",
    "    with open('dependencies/ground_truth_labels/' + map_name_in_strec + '.json') as f:\n",
    "        gt_labels_tmp = json.load(f)\n",
    "        gt_labels = pd.DataFrame([\n",
    "            {\n",
    "                'all_points_x': obs['shape_attributes']['all_points_x'],\n",
    "                'all_points_y': obs['shape_attributes']['all_points_y'],\n",
    "                'annotation': obs['region_attributes']['annotation'],\n",
    "                'multiline_g': obs['region_attributes'].get('multiline_g', None)\n",
    "            }\n",
    "            for obs in gt_labels_tmp[list(gt_labels_tmp.keys())[0]]['regions']\n",
    "        ])\n",
    "\n",
    "    if multiline_handling == 'largest':\n",
    "        gt_labels['annotation_length'] = gt_labels['annotation'].apply(len)\n",
    "        tmp1 = gt_labels[gt_labels['multiline_g'].isnull()]\n",
    "        tmp2 = gt_labels.dropna(subset=['multiline_g'])\n",
    "        gt_labels = pd.concat([tmp2.loc[tmp2.groupby('multiline_g')['annotation_length'].idxmax()], tmp1])\n",
    "    elif multiline_handling == 'components':\n",
    "        gt_labels['annotation_length'] = gt_labels['annotation'].apply(len)\n",
    "        tmp1 = gt_labels[gt_labels['multiline_g'].isnull()]\n",
    "        tmp2 = gt_labels.dropna(subset=['multiline_g'])\n",
    "        gt_labels = pd.concat([tmp2.loc[~tmp2.index.isin(tmp2.groupby('multiline_g')['annotation_length'].idxmax())], tmp1])\n",
    "    return gt_labels\n",
    "\n",
    "## Retain a subset of labels based on crop coordinates\n",
    "def coords_fail_condition(list, direction_for_drop, value, baseline):\n",
    "    if baseline == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        if direction_for_drop == '<':\n",
    "            num_coords_broke_rule = sum([0 if coord < value else 1 for coord in list])\n",
    "        elif direction_for_drop == '>':\n",
    "            num_coords_broke_rule = sum([0 if coord > value else 1 for coord in list])\n",
    "        if num_coords_broke_rule > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "def retain_crop_labels_only(df, left_x, right_x, top_y, bottom_y):\n",
    "    df['drop'] = 0\n",
    "    df['drop'] = df.apply(lambda row: coords_fail_condition(row['all_points_x'], '>', left_x, row['drop']), axis=1)\n",
    "    df['drop'] = df.apply(lambda row: coords_fail_condition(row['all_points_x'], '<', right_x, row['drop']), axis=1)\n",
    "    df['drop'] = df.apply(lambda row: coords_fail_condition(row['all_points_y'], '>', top_y, row['drop']), axis=1)\n",
    "    df['drop'] = df.apply(lambda row: coords_fail_condition(row['all_points_y'], '<', bottom_y, row['drop']), axis=1)\n",
    "    df = df[df['drop'] == 0]\n",
    "    print(\"retaining \" + str(len(df)) + \" labels fully inside crop area\")\n",
    "    return df\n",
    "\n",
    "## Calculate and Match IoUs\n",
    "def calculate_IoU_matrix(spotter_polygons, gt_polygons):\n",
    "    IoU_matrix = []\n",
    "    for sptr_poly in spotter_polygons:\n",
    "        row = []\n",
    "        for gt_poly in gt_polygons:\n",
    "            intersection_area = sptr_poly.intersection(gt_poly).area\n",
    "            union_area = sptr_poly.union(gt_poly).area\n",
    "            iou = intersection_area / union_area if union_area > 0 else 0\n",
    "            row.append(iou)\n",
    "        IoU_matrix.append(row)\n",
    "    return np.array(IoU_matrix)\n",
    "\n",
    "def maximize_1to1_precision(IoU_matrix):\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(IoU_matrix, maximize=True)\n",
    "    num_detected = IoU_matrix.shape[0]\n",
    "    IoU_pairs = IoU_matrix[row_ind, col_ind]\n",
    "    return num_detected, IoU_pairs\n",
    "\n",
    "def maximize_1to1_recall(IoU_matrix):\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(IoU_matrix, maximize=True)\n",
    "    num_gt = IoU_matrix.shape[1]\n",
    "    IoU_pairs = IoU_matrix[row_ind, col_ind]\n",
    "    return num_gt, IoU_pairs\n",
    "\n",
    "## Full pipeline \n",
    "def display_geographic_accuracy(map_name_in_strec, multiline_handling, spotter_layer):\n",
    "    print(\"\\n------\")\n",
    "    print(map_name_in_strec + \" | \" + multiline_handling + \" of multiline GT labels | spotter layer \" + spotter_layer)\n",
    "    print(\"------\")\n",
    "    gt_labels_full = load_ground_truth_labels(map_name_in_strec, multiline_handling)\n",
    "    gt_labels_crop = retain_crop_labels_only(gt_labels_full, left_x, right_x, top_y, bottom_y)\n",
    "    gt_labels_crop = cast_coords_as_Polygons(gt_labels_crop)\n",
    "    gt_polys = gt_labels_crop['label_polygons']\n",
    "\n",
    "    spotter_labels_full = load_spotter_labels(map_name_in_strec, spotter_layer)\n",
    "    spotter_labels_crop = retain_crop_labels_only(spotter_labels_full, left_x, right_x, top_y, bottom_y)\n",
    "    spotter_labels_crop = cast_coords_as_Polygons(spotter_labels_crop)\n",
    "    spotter_polys = spotter_labels_crop['label_polygons']\n",
    "\n",
    "    IoU_matrix = calculate_IoU_matrix(spotter_polys, gt_polys)\n",
    "    print(\"Avg of 1:1 IoUs: \" + str(maximize_1to1_precision(IoU_matrix)))\n",
    "    print(\"Avg of m:1 IoUs: \" + str(maximize_1to1_recall(IoU_matrix)))\n",
    "\n",
    "    return gt_labels_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average IoU for text bounding boxes; unmatched boxes included in denominators.\")\n",
    "print(\"m:1 -> multiple spotter text boxes can be matched with a single ground truth text box\")\n",
    "print(\"\")\n",
    "\n",
    "for layer_s in ['combined_tagged_all_layers_rectified']:\n",
    "    #display_geographic_accuracy(\"kiepert_1845\", \"largest\", layer_s + \".json\")\n",
    "    a = display_geographic_accuracy(\"kiepert_1845\", \"components\", layer_s + \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
