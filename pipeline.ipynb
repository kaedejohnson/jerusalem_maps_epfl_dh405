{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile #pip install Pillow==9.4.0\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import glob\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from ImageCrop import ImagePreprocessor\n",
    "from SpotterWrapper import Spotter, PolygonVisualizer\n",
    "from IPython.display import display\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Specify filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name folders for raw data and processed data\n",
    "map_data_topfolder = 'raw_maps_20231024'\n",
    "map_strec_topfolder = 'processed/strec'\n",
    "\n",
    "for fp in [map_strec_topfolder]:\n",
    "    if not os.path.isdir(fp):\n",
    "        os.makedirs(fp)\n",
    "\n",
    "# IMPORTANT! Locate spotter directory and detectron weights\n",
    "git_clone_location = 'C:/repo/'\n",
    "spotter_directory = git_clone_location + 'mapkurator-spotter/spotter-v2'\n",
    "model_weights = git_clone_location + 'detectron2-master/detectron2/checkpoint/model_v2_en.pth'\n",
    "spotter_config = spotter_directory + '/configs/PALEJUN/Finetune/Rumsey_Polygon_Finetune.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Crop all jpeg maps in (user defined) map_data_topfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_scan(img_path, output_dir, save_each_layer=False):\n",
    "    image = Image.open(img_path)\n",
    "    image_preprocessor = ImagePreprocessor(image, overlapping_tolerance=0.3, num_layers=5, min_patch_resolution=512, max_patch_resolution=4096)\n",
    "    image_preprocessor.process()\n",
    "    print(\"preprocessing done\")\n",
    "    spotter = Spotter(spotter_config, model_weights, confidence_thresh=0.8, draw_thresh=0.85)\n",
    "    all_layer_results = []\n",
    "\n",
    "    base_image_batch, base_offset_xs, base_offset_ys = image_preprocessor.get_image_patches(0)\n",
    "    vis = PolygonVisualizer()\n",
    "    vis.canvas_from_patches(base_image_batch, base_offset_xs, base_offset_ys)\n",
    "\n",
    "    for i in range(image_preprocessor.num_layers):\n",
    "        # If you want to save for each layer, uncomment the following line\n",
    "        # image_preprocessor.save_patches(os.path.join(output_dir, f'layer_{i}_patches'), layer=i)\n",
    "\n",
    "        image_batch, offset_xs, offset_ys = image_preprocessor.get_image_patches(i)\n",
    "        spotter.load_batch(image_batch, offset_xs, offset_ys)\n",
    "        results = spotter.inference_batch()\n",
    "        all_layer_results.extend(results)\n",
    "\n",
    "        #all_layer_offset_xs.extend(offset_xs)\n",
    "        #all_layer_offset_ys.extend(offset_ys)\n",
    "\n",
    "        if save_each_layer == True:\n",
    "            vis.draw(results).save(os.path.join(output_dir, f'combined_tagged_{i}.png'))\n",
    "            vis.save_json(results, os.path.join(output_dir, f'combined_tagged_{i}.json'))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    vis.draw(all_layer_results).save(os.path.join(output_dir, f'combined_tagged_all_layers.png'))\n",
    "    vis.save_json(all_layer_results, os.path.join(output_dir, f'combined_tagged_all_layers.json'))\n",
    "\n",
    "# Run crop on all maps\n",
    "for map_data_subfolder in next(os.walk(map_data_topfolder))[1]:\n",
    "    jpeg_list = glob.glob(map_data_topfolder + '/' + map_data_subfolder + '/*.jpeg')\n",
    "    if len(jpeg_list) != 1:\n",
    "        print(map_data_subfolder + \" failed. Please ensure there is exactly 1 file with extension .jpeg in the folder.\")\n",
    "    else:\n",
    "        map_image = jpeg_list[0].split(\"\\\\\")[1]\n",
    "        if map_data_subfolder in ['1846_vandevelde', '1874_saunders', '1845_kiepert']: # '1858_vandevelde', '1874_saunders', '1845_kiepert']: #,,]: #'1858_vandevelde', '1847_tobler', '1845_kiepert'\n",
    "            img_path = map_data_topfolder + '/' + map_data_subfolder + \"/\" + map_image\n",
    "            map_name = os.path.basename(img_path).split('.')[0] # get the map name without extension\n",
    "            output_dir = os.path.join(map_strec_topfolder, map_name)\n",
    "            if not os.path.isdir(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            pyramid_scan(img_path, output_dir, save_each_layer=False)\n",
    "            logging.info('Done cropping %s' %img_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Label Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import json \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from itertools import combinations\n",
    "import scipy\n",
    "import numpy as np\n",
    "import importlib\n",
    "import Clustering\n",
    "import TextRectify\n",
    "import json\n",
    "\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(TextRectify)\n",
    "\n",
    "map_name = '1846_vandevelde'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'processed/strec/{map_name}/combined_tagged_all_layers.json', 'r', encoding='utf-8') as f:\n",
    "\n",
    "    clustered = Clustering.cluster_polygons(json.load(f))\n",
    "\n",
    "    # visualize clusters\n",
    "    #image = Clustering.visualize_polygons(clustered, 'processed/strec/kiepert_1845/raw.jpeg')\n",
    "    #image.save('processed/strec/kiepert_1845/combined_tagged_all_layers_clustering.png')\n",
    "\n",
    "for label, cluster in clustered.items():\n",
    "    texts = []\n",
    "    scores = []\n",
    "    for polygon in cluster:\n",
    "        texts.append(polygon['text'])\n",
    "        scores.append(polygon['score'])\n",
    "\n",
    "    rectifier = TextRectify.TextRectifier(0.95, 0.5, 10, True, True)\n",
    "\n",
    "    rectifier.feed_data(texts, scores)\n",
    "\n",
    "    rectifier.fit()\n",
    "\n",
    "    rectified = rectifier.get_rectified_text()\n",
    "\n",
    "    if rectified is None:\n",
    "        rectified = max(texts, key=len)\n",
    "\n",
    "    for i in range(len(cluster)):\n",
    "        cluster[i]['text'] = rectified[i]\n",
    "\n",
    "#image = Clustering.visualize_polygons(clustered, 'processed/strec/kiepert_1845/raw.jpeg')\n",
    "#image.save('processed/strec/kiepert_1845/combined_tagged_all_layers_rectified.png')\n",
    "\n",
    "polygon_x = {}\n",
    "polygon_y = {}\n",
    "texts = {}\n",
    "scores = {}\n",
    "i = 0\n",
    "for label, cluster in clustered.items():\n",
    "    for polygon in cluster:\n",
    "        polygon_x[str(i)] = polygon['polygon_x']\n",
    "        polygon_y[str(i)] = polygon['polygon_y']\n",
    "        texts[str(i)] = polygon['text']\n",
    "        scores[str(i)] = polygon['score']\n",
    "        i += 1\n",
    "\n",
    "json_data = {'polygon_x': polygon_x, 'polygon_y': polygon_y, 'text': texts, 'score': scores}\n",
    "\n",
    "with open('processed/strec/kiepert_1845/combined_tagged_all_layers_rectified.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spotter_labels(map_name_in_strec, layer_json_w_extension='combined_tagged_all_layers.json'):\n",
    "    with open('processed/strec/' + map_name_in_strec + '/' + layer_json_w_extension) as f:\n",
    "        spotter_labels_tmp = json.load(f)\n",
    "    spotter_labels = pd.DataFrame(spotter_labels_tmp)\n",
    "    spotter_labels = spotter_labels.rename(columns={'polygon_x':'all_points_x','polygon_y':'all_points_y'})\n",
    "    return spotter_labels\n",
    "\n",
    "## Convert labels to polygon objects (df needs all_points_x and all_points_y cols)\n",
    "def create_polygon_object(x_coords, y_coords):\n",
    "    return Polygon(zip(x_coords, y_coords)).buffer(0) # buffer fixes errors that occur due to weird self-overlapping edges when manually labeling\n",
    "\n",
    "def cast_coords_as_Polygons(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.loc[:, 'label_polygons'] = df.apply(lambda row: create_polygon_object(row['all_points_x'], row['all_points_y']), axis=1)\n",
    "    return df_copy\n",
    "    \n",
    "def intersection_over_minimum(obj1, obj2):\n",
    "    if (isinstance(obj1, Polygon) or isinstance(obj1, MultiPolygon)) and (isinstance(obj2, Polygon) or isinstance(obj2, MultiPolygon)):\n",
    "        IoM = obj1.intersection(obj2).area / min(obj1.area, obj2.area)\n",
    "    elif isinstance(obj1, str) and isinstance(obj2, str):\n",
    "        obj1 = obj1.lower()\n",
    "        obj2 = obj2.lower()\n",
    "        cntr1 = Counter(obj1)\n",
    "        cntr2 = Counter(obj2)\n",
    "        global_char_set = set(cntr1.keys()) | set(cntr2.keys())\n",
    "        IoM = sum(min(cntr1[char], cntr2[char]) for char in global_char_set) / min(len(obj1), len(obj2))\n",
    "    else:\n",
    "        print(obj1, obj2)\n",
    "        print(\"both inputs must be of the same type (Polygon or string)\")\n",
    "        IoM = np.nan\n",
    "    return IoM\n",
    "\n",
    "def IoMs(label1, label2):\n",
    "    poly1 = label1[0]\n",
    "    text1 = label1[1]\n",
    "    poly2 = label2[0]\n",
    "    text2 = label2[1]\n",
    "    if len(set(text1.lower()) | set(text2.lower())) == 0:\n",
    "        return (0, 0)\n",
    "    if not poly1.intersects(poly2):\n",
    "        return (0, 0)\n",
    "    poly_IoU = intersection_over_minimum(poly1, poly2)\n",
    "    text_IoU = intersection_over_minimum(text1, text2)\n",
    "    return (poly_IoU, text_IoU)\n",
    "\n",
    "def update_P_matrix(df_w_labels, geo_threshold, text_threshold, P = None):\n",
    "    labels = df_w_labels['labels']\n",
    "    if P == None:\n",
    "        P = {}\n",
    "    to_combine = []\n",
    "    for i, j in combinations(labels.index, 2):\n",
    "        if i not in P.keys():\n",
    "            P[i] = {}\n",
    "        if j in P[i].keys():\n",
    "            pass\n",
    "        else:\n",
    "            similarity = IoMs(labels[i], labels[j])\n",
    "            P[i][j] = similarity\n",
    "            if similarity[0] > geo_threshold and similarity[1] > text_threshold:\n",
    "                to_combine.append((i,j))\n",
    "    return P, to_combine\n",
    "\n",
    "def combine_labels(label1, label2):\n",
    "    poly1 = label1[0]\n",
    "    text1 = label1[1]\n",
    "    poly2 = label2[0]\n",
    "    text2 = label2[1]\n",
    "    poly_new = poly1.union(poly2)\n",
    "    text_new = text1 if len(text1) > len(text2) else text2\n",
    "    return (poly_new, text_new)\n",
    "\n",
    "def amalgamate_labels(df, P, to_combine):\n",
    "    for pair in to_combine:\n",
    "        if str(pair[0]) in df.index and str(pair[1]) in df.index:\n",
    "            new_label = combine_labels(df.loc[str(pair[0])]['labels'], df.loc[str(pair[1])]['labels'])\n",
    "            df.loc[str(int(df.index[-1]) + 1)] = [new_label]\n",
    "            df = df.drop([str(pair[0])]).copy()\n",
    "            df = df.drop([str(pair[1])]).copy()\n",
    "            try:\n",
    "                P.pop(pair[0])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                P.pop(pair[1])\n",
    "            except:\n",
    "                pass\n",
    "        else: # one of the polygons has already been swallowed so no combination can no longer occur\n",
    "            pass\n",
    "    return df, P\n",
    "\n",
    "def amalgamate_labels_wrapper(df, geo_threshold, text_threshold):\n",
    "    pre_amal = 0\n",
    "    post_amal = len(df)\n",
    "    P = None\n",
    "    while pre_amal - post_amal != 0:\n",
    "        pre_amal = post_amal\n",
    "        P, to_combine = update_P_matrix(df, geo_threshold, text_threshold, P)\n",
    "        print(str(pre_amal) + \" labels.\")\n",
    "        df, P = amalgamate_labels(df, P, to_combine)\n",
    "        post_amal = len(df)\n",
    "    print(\"Amalgamation completed with \" + str(pre_amal) + \" labels.\")\n",
    "    return df\n",
    "\n",
    "df = load_spotter_labels(\"kiepert_1845\", \"combined_tagged_all_layers_rectified.json\")\n",
    "df = cast_coords_as_Polygons(df)\n",
    "df['labels'] = df.apply(lambda row: (row['label_polygons'], row['text']), axis=1)\n",
    "df = df[['labels']]\n",
    "df = amalgamate_labels_wrapper(df, 0.75, .5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('amalgamate.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('amalgamate.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list(df[\"labels\"])\n",
    "polygons = []\n",
    "texts = []\n",
    "\n",
    "import shapely as sh\n",
    "import shapely.geometry as sg\n",
    "\n",
    "#result[0][0].exterior.coords.xy[0]\n",
    "\n",
    "for i in range(len(result)):\n",
    "    poly = result[i][0]\n",
    "    polygons.append(poly)\n",
    "    texts.append(result[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload SpotterWrapper module\n",
    "import importlib\n",
    "import SpotterWrapper\n",
    "importlib.reload(SpotterWrapper)\n",
    "\n",
    "vis = SpotterWrapper.PolygonVisualizer()\n",
    "canvas = Image.open('processed/strec/kiepert_1845/raw.jpeg')\n",
    "vis.canvas_from_image(canvas)\n",
    "\n",
    "vis.draw_poly(polygons, texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import json \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from itertools import combinations\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def visualize_crop(map_name_in_strec, raw_or_spotter, left_x, right_x, top_y, bottom_y):\n",
    "    if raw_or_spotter == \"raw\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/raw.jpeg') \n",
    "    elif raw_or_spotter == \"spotter_0\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_0.png')\n",
    "    elif raw_or_spotter == \"spotter_1\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_1.png')\n",
    "    elif raw_or_spotter == \"spotter_2\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_2.png')\n",
    "    elif raw_or_spotter == \"all\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_all_layers.png')\n",
    "    width, height = map_img.size\n",
    "    print(\"full map is \" + str(width) + \" pixels wide by \" + str(height) + \" pixels high.\\n displaying crop:\")\n",
    "    display(map_img.crop((left_x, top_y, right_x, bottom_y, )))\n",
    "\n",
    "left_x = 2475\n",
    "right_x = 3550\n",
    "top_y = 4820\n",
    "bottom_y = 5850\n",
    "\n",
    "#visualize_crop(\"kiepert_1845\", \"all\", left_x, right_x, top_y, bottom_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "## Load in (1) ground truth labels that were built using Via and (2) spotter labels\n",
    "def load_ground_truth_labels(map_name_in_strec, multiline_handling, labels_on_fullsize_map=True):\n",
    "    with open('dependencies/ground_truth_labels/' + map_name_in_strec + '.json') as f:\n",
    "        gt_labels_tmp = json.load(f)\n",
    "        gt_labels = pd.DataFrame([\n",
    "            {\n",
    "                'all_points_x': obs['shape_attributes']['all_points_x'],\n",
    "                'all_points_y': obs['shape_attributes']['all_points_y'],\n",
    "                'annotation': obs['region_attributes']['annotation'],\n",
    "                'multiline_g': obs['region_attributes'].get('multiline_g', None)\n",
    "            }\n",
    "            for obs in gt_labels_tmp[list(gt_labels_tmp.keys())[0]]['regions']\n",
    "        ])\n",
    "\n",
    "    if multiline_handling == 'largest':\n",
    "        gt_labels['annotation_length'] = gt_labels['annotation'].apply(len)\n",
    "        tmp1 = gt_labels[gt_labels['multiline_g'].isnull()]\n",
    "        tmp2 = gt_labels.dropna(subset=['multiline_g'])\n",
    "        gt_labels = pd.concat([tmp2.loc[tmp2.groupby('multiline_g')['annotation_length'].idxmax()], tmp1])\n",
    "    elif multiline_handling == 'components':\n",
    "        gt_labels['annotation_length'] = gt_labels['annotation'].apply(len)\n",
    "        tmp1 = gt_labels[gt_labels['multiline_g'].isnull()]\n",
    "        tmp2 = gt_labels.dropna(subset=['multiline_g'])\n",
    "        gt_labels = pd.concat([tmp2.loc[~tmp2.index.isin(tmp2.groupby('multiline_g')['annotation_length'].idxmax())], tmp1])\n",
    "    return gt_labels\n",
    "\n",
    "## Retain a subset of labels based on crop coordinates\n",
    "def coords_fail_condition(list, direction_for_drop, value, baseline):\n",
    "    if baseline == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        if direction_for_drop == '<':\n",
    "            num_coords_broke_rule = sum([0 if coord < value else 1 for coord in list])\n",
    "        elif direction_for_drop == '>':\n",
    "            num_coords_broke_rule = sum([0 if coord > value else 1 for coord in list])\n",
    "        if num_coords_broke_rule > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "def retain_crop_labels_only(df, left_x, right_x, top_y, bottom_y):\n",
    "    df['drop'] = 0\n",
    "    df['drop'] = df.apply(lambda row: coords_fail_condition(row['all_points_x'], '>', left_x, row['drop']), axis=1)\n",
    "    df['drop'] = df.apply(lambda row: coords_fail_condition(row['all_points_x'], '<', right_x, row['drop']), axis=1)\n",
    "    df['drop'] = df.apply(lambda row: coords_fail_condition(row['all_points_y'], '>', top_y, row['drop']), axis=1)\n",
    "    df['drop'] = df.apply(lambda row: coords_fail_condition(row['all_points_y'], '<', bottom_y, row['drop']), axis=1)\n",
    "    df = df[df['drop'] == 0]\n",
    "    print(\"retaining \" + str(len(df)) + \" labels fully inside crop area\")\n",
    "    return df\n",
    "\n",
    "## Calculate and Match IoUs\n",
    "def calculate_IoU_matrix(spotter_polygons, gt_polygons):\n",
    "    IoU_matrix = []\n",
    "    for sptr_poly in spotter_polygons:\n",
    "        row = []\n",
    "        for gt_poly in gt_polygons:\n",
    "            intersection_area = sptr_poly.intersection(gt_poly).area\n",
    "            union_area = sptr_poly.union(gt_poly).area\n",
    "            iou = intersection_area / union_area if union_area > 0 else 0\n",
    "            row.append(iou)\n",
    "        IoU_matrix.append(row)\n",
    "    return np.array(IoU_matrix)\n",
    "\n",
    "def maximize_1to1_global_average(IoU_matrix):\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(IoU_matrix, maximize=True)\n",
    "    return IoU_matrix[row_ind, col_ind].sum() / (IoU_matrix.shape[0] + IoU_matrix.shape[0] - 2 * len(row_ind))\n",
    "\n",
    "def maximize_mto1_global_average(IoU_matrix):\n",
    "    return np.sum(np.max(IoU_matrix, axis=1)) / IoU_matrix.shape[0]\n",
    "\n",
    "## Full pipeline \n",
    "def display_geographic_accuracy(map_name_in_strec, multiline_handling, spotter_layer):\n",
    "    print(\"\\n------\")\n",
    "    print(map_name_in_strec + \" | \" + multiline_handling + \" of multiline GT labels | spotter layer \" + spotter_layer)\n",
    "    print(\"------\")\n",
    "    gt_labels_full = load_ground_truth_labels(map_name_in_strec, multiline_handling)\n",
    "    gt_labels_crop = retain_crop_labels_only(gt_labels_full, left_x, right_x, top_y, bottom_y)\n",
    "    gt_labels_crop = cast_coords_as_Polygons(gt_labels_crop)\n",
    "    gt_polys = gt_labels_crop['label_polygons']\n",
    "\n",
    "    spotter_labels_full = load_spotter_labels(map_name_in_strec, spotter_layer)\n",
    "    spotter_labels_crop = retain_crop_labels_only(spotter_labels_full, left_x, right_x, top_y, bottom_y)\n",
    "    spotter_labels_crop = cast_coords_as_Polygons(spotter_labels_crop)\n",
    "    spotter_polys = spotter_labels_crop['label_polygons']\n",
    "\n",
    "    IoU_matrix = calculate_IoU_matrix(spotter_polys, gt_polys)\n",
    "    print(\"Avg of 1:1 IoUs: \" + str(maximize_1to1_global_average(IoU_matrix)))\n",
    "    print(\"Avg of m:1 IoUs: \" + str(maximize_mto1_global_average(IoU_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average IoU for text bounding boxes; unmatched boxes included in denominators.\")\n",
    "print(\"m:1 -> multiple spotter text boxes can be matched with a single ground truth text box\")\n",
    "print(\"\")\n",
    "\n",
    "for layer_s in ['combined_tagged_0', 'combined_tagged_1', 'combined_tagged_2', 'combined_tagged_all_layers']:\n",
    "    display_geographic_accuracy(\"kiepert_1845\", \"largest\", layer_s + \".json\")\n",
    "    display_geographic_accuracy(\"kiepert_1845\", \"components\", layer_s + \".json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
