{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile #pip install Pillow==9.4.0\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import glob\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from ImageCrop import ImagePreprocessor\n",
    "from SpotterWrapper import Spotter, PolygonVisualizer\n",
    "from IPython.display import display\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Specify filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name folders for raw data and processed data\n",
    "map_data_topfolder = 'raw_maps_20231024'\n",
    "map_strec_topfolder = 'processed/strec'\n",
    "\n",
    "for fp in [map_strec_topfolder]:\n",
    "    if not os.path.isdir(fp):\n",
    "        os.makedirs(fp)\n",
    "\n",
    "# IMPORTANT! Locate spotter directory and detectron weights\n",
    "git_clone_location = 'C:/repo/'\n",
    "spotter_directory = git_clone_location + 'mapkurator-spotter/spotter-v2'\n",
    "model_weights = git_clone_location + 'detectron2-master/detectron2/checkpoint/model_v2_en.pth'\n",
    "spotter_config = spotter_directory + '/configs/PALEJUN/Finetune/Rumsey_Polygon_Finetune.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Crop all jpeg maps in (user defined) map_data_topfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_scan(img_path, output_dir, save_each_layer=False):\n",
    "    image = Image.open(img_path)\n",
    "    image_preprocessor = ImagePreprocessor(image, overlapping_tolerance=0.3, num_layers=5, min_patch_resolution=512, max_patch_resolution=4096)\n",
    "    image_preprocessor.process()\n",
    "    print(\"preprocessing done\")\n",
    "    spotter = Spotter(spotter_config, model_weights, confidence_thresh=0.7, draw_thresh=0.85)\n",
    "    all_layer_results = []\n",
    "\n",
    "    base_image_batch, base_offset_xs, base_offset_ys = image_preprocessor.get_image_patches(0)\n",
    "    vis = PolygonVisualizer()\n",
    "    vis.canvas_from_patches(base_image_batch, base_offset_xs, base_offset_ys)\n",
    "\n",
    "    for i in range(image_preprocessor.num_layers):\n",
    "        # If you want to save for each layer, uncomment the following line\n",
    "        # image_preprocessor.save_patches(os.path.join(output_dir, f'layer_{i}_patches'), layer=i)\n",
    "\n",
    "        image_batch, offset_xs, offset_ys = image_preprocessor.get_image_patches(i)\n",
    "        spotter.load_batch(image_batch, offset_xs, offset_ys)\n",
    "        results = spotter.inference_batch()\n",
    "        all_layer_results.extend(results)\n",
    "\n",
    "        #all_layer_offset_xs.extend(offset_xs)\n",
    "        #all_layer_offset_ys.extend(offset_ys)\n",
    "\n",
    "        if save_each_layer == True:\n",
    "            vis.draw(results).save(os.path.join(output_dir, f'combined_tagged_{i}.png'))\n",
    "            vis.save_json(results, os.path.join(output_dir, f'combined_tagged_{i}.json'))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    vis.draw(all_layer_results).save(os.path.join(output_dir, f'combined_tagged_all_layers.png'))\n",
    "    vis.save_json(all_layer_results, os.path.join(output_dir, f'combined_tagged_all_layers.json'))\n",
    "\n",
    "# Run crop on all maps\n",
    "for map_data_subfolder in next(os.walk(map_data_topfolder))[1]:\n",
    "    jpeg_list = glob.glob(map_data_topfolder + '/' + map_data_subfolder + '/*.jpeg')\n",
    "    if len(jpeg_list) != 1:\n",
    "        print(map_data_subfolder + \" failed. Please ensure there is exactly 1 file with extension .jpeg in the folder.\")\n",
    "    else:\n",
    "        map_image = jpeg_list[0].split(\"\\\\\")[1]\n",
    "        if map_data_subfolder in ['1846_vandevelde', '1874_saunders', '1845_kiepert']: # '1858_vandevelde', '1874_saunders', '1845_kiepert']: #,,]: #'1858_vandevelde', '1847_tobler', '1845_kiepert'\n",
    "            img_path = map_data_topfolder + '/' + map_data_subfolder + \"/\" + map_image\n",
    "            map_name = os.path.basename(img_path).split('.')[0] # get the map name without extension\n",
    "            output_dir = os.path.join(map_strec_topfolder, map_name)\n",
    "            if not os.path.isdir(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            pyramid_scan(img_path, output_dir, save_each_layer=False)\n",
    "            logging.info('Done cropping %s' %img_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Label Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import json \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "import Clustering\n",
    "import TextRectify\n",
    "import TextAmalgamate\n",
    "import ExtractHandling\n",
    "import json\n",
    "import pickle\n",
    "import SpotterWrapper\n",
    "import Grouping\n",
    "import BezierSplineMetric\n",
    "import FontSimilarity\n",
    "import SequenceRecovery\n",
    "import SubwordDeduplication as sd\n",
    "importlib.reload(SpotterWrapper)\n",
    "importlib.reload(Grouping)\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(TextRectify)\n",
    "importlib.reload(TextAmalgamate)\n",
    "importlib.reload(ExtractHandling)\n",
    "importlib.reload(BezierSplineMetric)\n",
    "importlib.reload(FontSimilarity)\n",
    "importlib.reload(SequenceRecovery)\n",
    "\n",
    "map_name_in_strec = 'vandevelde_1846' # 'kiepert_1845', 'saunders_1874', 'vandevelde_1846'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Subword Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "971 polygons kept.\n"
     ]
    }
   ],
   "source": [
    "sd.subword_deduplication(map_name_in_strec, do_cluster_pre_merge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Nested Word Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\26553\\anaconda3\\envs\\mapKurator\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1983: ShapelyDeprecationWarning: __len__ for multi-part geometries is deprecated and will be removed in Shapely 2.0. Check the length of the `geoms` property instead to get the  number of parts of a multi-part geometry.\n",
      "  result[:] = values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "971 labels.\n",
      "894 labels.\n",
      "871 labels.\n",
      "868 labels.\n",
      "867 labels.\n",
      "Amalgamation completed with 867 labels.\n"
     ]
    }
   ],
   "source": [
    "# Amalgamation stage - assumes there exists \"combined_tagged_all_layers_rectified_premerge.json\" in map_name_in_strec processed folder.\n",
    "df = ExtractHandling.prepare_labels_for_amalgamation(map_name_in_strec)\n",
    "df = TextAmalgamate.amalgamate_labels_wrapper(df, 0.75, .5)\n",
    "\n",
    "# Save amalgamated labels\n",
    "with open(f'processed/strec/{map_name_in_strec}/deduplicated_flattened_labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Multi-Word Sequence Recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Prepare by calculating spline and font metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('processed/strec/' + map_name_in_strec + '/deduplicated_flattened_labels.pickle', 'rb'))\n",
    "df['polygons'] = df['labels'].apply(lambda x: x[0])\n",
    "df['texts'] = df['labels'].apply(lambda x: x[1])\n",
    "\n",
    "# Uncomment to draw splines later\n",
    "## BezierSplineMetric.draw_splines(map_name_in_strec, polygons, texts, PCA_features, all_splines)\n",
    "\n",
    "# reset index so list-based operations match df index\n",
    "df = df.reset_index(drop=True).copy()\n",
    "\n",
    "# pca for principal directions\n",
    "df['PCA_features'] = Grouping.calc_PCA_feats(df['polygons'], do_separation=True, enhance_coords=True)\n",
    "\n",
    "# find neighbors for spline and font metric consideration\n",
    "df = BezierSplineMetric.calc_neighbours(df, radius_multiplier = 40)\n",
    "\n",
    "# calculate spline metric between identified neighbors\n",
    "df = BezierSplineMetric.spline_metric(df)\n",
    "\n",
    "# calculate font metric between identified neighbors - long due to need to work with images\n",
    "df = FontSimilarity.calc_font_similarities(df, map_name_in_strec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'processed/strec/{map_name_in_strec}/seq_rec_prepared_labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional - draw splines\n",
    "#df = pickle.load(open(f'processed/strec/{map_name_in_strec}/seq_rec_prepared_labels.pickle', 'rb'))\n",
    "#BezierSplineMetric.draw_splines(map_name_in_strec, df['polygons'].tolist(), df['texts'].tolist(), df['PCA_features'].tolist(), df['all_splines'].explode().dropna().tolist(), spline_metric_threshold = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Iterative Sequence Recovery - not yet improving results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open(f'processed/strec/{map_name_in_strec}/seq_rec_prepared_labels.pickle', 'rb'))\n",
    "# Drop PCA_features - no longer needed, makes me feel good to discard stuff i don't need\n",
    "df.drop('PCA_features', axis=1, inplace=True)\n",
    "df.drop('all_splines', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867 labels.\n",
      "812 labels.\n",
      "811 labels.\n",
      "Sequence Recovery completed with 811 labels.\n"
     ]
    }
   ],
   "source": [
    "df = SequenceRecovery.sl_sequence_recovery_wrapper(df, .9, .05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save amalgamated labels\n",
    "with open(f'processed/strec/{map_name_in_strec}/fully_processed_labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import scipy\n",
    "import numpy as np\n",
    "import importlib \n",
    "\n",
    "import Evaluation\n",
    "importlib.reload(Evaluation)\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: Isolate crops to be used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_crop(map_name_in_strec, raw_or_spotter, left_x, right_x, top_y, bottom_y):\n",
    "    if raw_or_spotter == \"raw\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/raw.jpeg') \n",
    "    elif raw_or_spotter == \"spotter_0\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_0.png')\n",
    "    elif raw_or_spotter == \"spotter_1\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_1.png')\n",
    "    elif raw_or_spotter == \"spotter_2\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_2.png')\n",
    "    elif raw_or_spotter == \"all\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_all_layers.png')\n",
    "    elif raw_or_spotter == \"rectified\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_all_layers_rectified.png')\n",
    "    width, height = map_img.size\n",
    "    print(\"full map is \" + str(width) + \" pixels wide by \" + str(height) + \" pixels high.\\n displaying crop:\")\n",
    "    display(map_img.crop((left_x, top_y, right_x, bottom_y, )))\n",
    "\n",
    "left_x = 2475\n",
    "right_x = 3550\n",
    "top_y = 4820\n",
    "bottom_y = 5850\n",
    "\n",
    "#visualize_crop(\"kiepert_1845\", \"all\", left_x, right_x, top_y, bottom_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Precision and Recall: 1:1 Matching on Geometry, then IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kiepert baseline (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.19116442583187376\n",
      "Avg of Text Precision: 0.08627091726132564\n",
      "Avg of Geographic Recall: 0.14226189829348745\n",
      "Avg of Text Recall: 0.06420161284563769\n",
      "\n",
      "kiepert pyramid - subword dedup, nested word flattening (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.3178650375027149\n",
      "Avg of Text Precision: 0.28850633486739347\n",
      "Avg of Geographic Recall: 0.547023552911649\n",
      "Avg of Text Recall: 0.49649927395783994\n",
      "\n",
      "kiepert pyramid - subword dedup, nested word flattening, sequence recovery (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.376269773036337\n",
      "Avg of Text Precision: 0.3577637254442061\n",
      "Avg of Geographic Recall: 0.5862808091496414\n",
      "Avg of Text Recall: 0.5574458047619025\n",
      "\n",
      "vandevelde baseline (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.149042649550399\n",
      "Avg of Text Precision: 0.09590009588158856\n",
      "Avg of Geographic Recall: 0.3567510228599976\n",
      "Avg of Text Recall: 0.22954810184422794\n",
      "\n",
      "vandevelde pyramid - subword dedup, nested word flattening (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.2921573874252001\n",
      "Avg of Text Precision: 0.2869855380644992\n",
      "Avg of Geographic Recall: 0.5532342017200598\n",
      "Avg of Text Recall: 0.5434406997391581\n",
      "\n",
      "vandevelde pyramid - subword dedup, nested word flattening, sequence recovery (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.3305395198548378\n",
      "Avg of Text Precision: 0.33176684927106087\n",
      "Avg of Geographic Recall: 0.5837187265521604\n",
      "Avg of Text Recall: 0.5858861380744267\n",
      "\n",
      "saunders baseline (gt = components)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot set a frame with no defined index and a scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m v123_geo_prec, v123_text_prec, v123_geo_rec, v123_text_rec, v123_IoU_pairs, v123_num_detected, v123_num_gt \u001b[38;5;241m=\u001b[39m Evaluation\u001b[38;5;241m.\u001b[39mprec_rec(vname, multiline_handling, vandevelde_gt_patches, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethods_1_2_3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124msaunders baseline (gt = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m multiline_handling \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m sbase_geo_prec, sbase_text_prec, sbase_geo_rec, sbase_text_rec, sbase_IoU_pairs, sbase_num_detected, sbase_num_gt \u001b[38;5;241m=\u001b[39m \u001b[43mEvaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprec_rec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiline_handling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaunders_gt_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethods_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124msaunders pyramid - subword dedup, nested word flattening (gt = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m multiline_handling \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m s12_geo_prec, s12_text_prec, s12_geo_rec, s12_text_rec, s12_IoU_pairs, s12_num_detected, s12_num_gt \u001b[38;5;241m=\u001b[39m Evaluation\u001b[38;5;241m.\u001b[39mprec_rec(vname, multiline_handling, saunders_gt_patches, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethods_1_2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\Documents\\GitHub\\jerusalem_maps_epfl_dh405\\Evaluation.py:208\u001b[0m, in \u001b[0;36mprec_rec\u001b[1;34m(map_name_in_strec, multiline_handling, patches, methods)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprec_rec\u001b[39m(map_name_in_strec, multiline_handling, patches, methods \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethods_1_2_3\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 208\u001b[0m     num_detected, num_gt, IoU_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mgeographic_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_name_in_strec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiline_handling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m     IoU_pairs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(IoU_pairs, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeo_IoU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspotter_txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgt_txt\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    211\u001b[0m     IoU_pairs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalized_txt_similarity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m IoU_pairs\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: text_compare(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspotter_txt\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgt_txt\u001b[39m\u001b[38;5;124m'\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\Documents\\GitHub\\jerusalem_maps_epfl_dh405\\Evaluation.py:154\u001b[0m, in \u001b[0;36mgeographic_evaluation\u001b[1;34m(map_name_in_strec, multiline_handling, patches, methods)\u001b[0m\n\u001b[0;32m    152\u001b[0m gt_labels_full \u001b[38;5;241m=\u001b[39m load_ground_truth_labels(map_name_in_strec, multiline_handling) \u001b[38;5;66;03m# Evaluation.load_ground_truth_labels(map_name_in_strec, \"components\")\u001b[39;00m\n\u001b[0;32m    153\u001b[0m gt_labels_crop \u001b[38;5;241m=\u001b[39m retain_crop_coords_only(gt_labels_full, left_x, right_x, top_y, bottom_y) \n\u001b[1;32m--> 154\u001b[0m gt_labels_crop \u001b[38;5;241m=\u001b[39m \u001b[43mretain_alphabetic_annotations_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_labels_crop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m gt_labels_crop \u001b[38;5;241m=\u001b[39m ExtractHandling\u001b[38;5;241m.\u001b[39mcast_coords_as_Polygons(gt_labels_crop) \u001b[38;5;66;03m#ExtractHandling.cast_coords_as_Polygons(gt_labels_full)\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m methods \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethods_0\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\Documents\\GitHub\\jerusalem_maps_epfl_dh405\\Evaluation.py:116\u001b[0m, in \u001b[0;36mretain_alphabetic_annotations_only\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretain_alphabetic_annotations_only\u001b[39m(df):\n\u001b[0;32m    115\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 116\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdrop_txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    117\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    118\u001b[0m     df\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation_stripped\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(remove_non_alphabetical_characters_and_accents)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\mapKurator\\lib\\site-packages\\pandas\\core\\indexing.py:716\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    715\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 716\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\mapKurator\\lib\\site-packages\\pandas\\core\\indexing.py:1615\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj):\n\u001b[0;32m   1614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like_indexer(value):\n\u001b[1;32m-> 1615\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1616\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot set a frame with no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1617\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefined index and a scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1618\u001b[0m         )\n\u001b[0;32m   1619\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot set a frame with no defined index and a scalar"
     ]
    }
   ],
   "source": [
    "kiepert_gt_patches = [[2475, 3550, 4820, 5850]]\n",
    "saunders_gt_patches = [[3150, 4150, 2250, 3250], [6750, 7750, 2250, 3250], [5400, 6400, 4500, 5500], [7650, 8650, 5400, 6400], [7650, 8650, 3150, 4150]]\n",
    "vandevelde_gt_patches = [[2850, 5250, 1450, 3850]]\n",
    "\n",
    "# Gimme them numbers :)\n",
    "\n",
    "kname = \"kiepert_1845\"\n",
    "vname = \"vandevelde_1846\"\n",
    "sname = \"saunders_1846\"\n",
    "\n",
    "multiline_handling = \"components\" # \"largest\" for multiline gt\n",
    "\n",
    "print(\"\\nkiepert baseline (gt = \" + multiline_handling + \")\\n\")\n",
    "kbase_geo_prec, kbase_text_prec, kbase_geo_rec, kbase_text_rec, kbase_IoU_pairs, kbase_num_detected, kbase_num_gt = Evaluation.prec_rec(kname, multiline_handling, kiepert_gt_patches, \"methods_0\")\n",
    "print(\"\\nkiepert pyramid - subword dedup, nested word flattening (gt = \" + multiline_handling + \")\\n\")\n",
    "k12_geo_prec, k12_text_prec, k12_geo_rec, k12_text_rec, k12_IoU_pairs, k12_num_detected, k12_num_gt = Evaluation.prec_rec(kname, multiline_handling, kiepert_gt_patches, \"methods_1_2\")\n",
    "print(\"\\nkiepert pyramid - subword dedup, nested word flattening, sequence recovery (gt = \" + multiline_handling + \")\\n\")\n",
    "k123_geo_prec, k123_text_prec, k123_geo_rec, k123_text_rec, k123_IoU_pairs, k123_num_detected, k123_num_gt = Evaluation.prec_rec(kname, multiline_handling, kiepert_gt_patches, \"methods_1_2_3\")\n",
    "\n",
    "print(\"\\nvandevelde baseline (gt = \" + multiline_handling + \")\\n\")\n",
    "vbase_geo_prec, vbase_text_prec, vbase_geo_rec, vbase_text_rec, vbase_IoU_pairs, vbase_num_detected, vbase_num_gt = Evaluation.prec_rec(vname, multiline_handling, vandevelde_gt_patches, \"methods_0\")\n",
    "print(\"\\nvandevelde pyramid - subword dedup, nested word flattening (gt = \" + multiline_handling + \")\\n\")\n",
    "v12_geo_prec, v12_text_prec, v12_geo_rec, v12_text_rec, v12_IoU_pairs, v12_num_detected, v12_num_gt = Evaluation.prec_rec(vname, multiline_handling, vandevelde_gt_patches, \"methods_1_2\")\n",
    "print(\"\\nvandevelde pyramid - subword dedup, nested word flattening, sequence recovery (gt = \" + multiline_handling + \")\\n\")\n",
    "v123_geo_prec, v123_text_prec, v123_geo_rec, v123_text_rec, v123_IoU_pairs, v123_num_detected, v123_num_gt = Evaluation.prec_rec(vname, multiline_handling, vandevelde_gt_patches, \"methods_1_2_3\")\n",
    "\n",
    "print(\"\\nsaunders baseline (gt = \" + multiline_handling + \")\\n\")\n",
    "sbase_geo_prec, sbase_text_prec, sbase_geo_rec, sbase_text_rec, sbase_IoU_pairs, sbase_num_detected, sbase_num_gt = Evaluation.prec_rec(vname, multiline_handling, saunders_gt_patches, \"methods_0\")\n",
    "print(\"\\nsaunders pyramid - subword dedup, nested word flattening (gt = \" + multiline_handling + \")\\n\")\n",
    "s12_geo_prec, s12_text_prec, s12_geo_rec, s12_text_rec, s12_IoU_pairs, s12_num_detected, s12_num_gt = Evaluation.prec_rec(vname, multiline_handling, saunders_gt_patches, \"methods_1_2\")\n",
    "print(\"\\nsaunders pyramid - subword dedup, nested word flattening, sequence recovery (gt = \" + multiline_handling + \")\\n\")\n",
    "s123_geo_prec, s123_text_prec, s123_geo_rec, s123_text_rec, s123_IoU_pairs, s123_num_detected, s123_num_gt = Evaluation.prec_rec(vname, multiline_handling, saunders_gt_patches, \"methods_1_2_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
