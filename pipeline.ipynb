{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile #pip install Pillow==9.4.0\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import glob\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from ImageCrop import ImagePreprocessor\n",
    "from SpotterWrapper import Spotter, PolygonVisualizer\n",
    "from IPython.display import display\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Specify filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name folders for raw data and processed data\n",
    "map_data_topfolder = 'raw_maps_20231024'\n",
    "map_strec_topfolder = 'processed/strec'\n",
    "\n",
    "for fp in [map_strec_topfolder]:\n",
    "    if not os.path.isdir(fp):\n",
    "        os.makedirs(fp)\n",
    "\n",
    "# IMPORTANT! Locate spotter directory and detectron weights\n",
    "git_clone_location = 'C:/repo/'\n",
    "spotter_directory = git_clone_location + 'mapkurator-spotter/spotter-v2'\n",
    "model_weights = git_clone_location + 'detectron2-master/detectron2/checkpoint/model_v2_en.pth'\n",
    "spotter_config = spotter_directory + '/configs/PALEJUN/Finetune/Rumsey_Polygon_Finetune.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Crop all jpeg maps in (user defined) map_data_topfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_scan(img_path, output_dir, save_each_layer=False):\n",
    "    image = Image.open(img_path)\n",
    "    image_preprocessor = ImagePreprocessor(image, overlapping_tolerance=0.3, num_layers=5, min_patch_resolution=512, max_patch_resolution=4096)\n",
    "    image_preprocessor.process()\n",
    "    print(\"preprocessing done\")\n",
    "    spotter = Spotter(spotter_config, model_weights, confidence_thresh=0.8, draw_thresh=0.85)\n",
    "    all_layer_results = []\n",
    "\n",
    "    base_image_batch, base_offset_xs, base_offset_ys = image_preprocessor.get_image_patches(0)\n",
    "    vis = PolygonVisualizer()\n",
    "    vis.canvas_from_patches(base_image_batch, base_offset_xs, base_offset_ys)\n",
    "\n",
    "    for i in range(image_preprocessor.num_layers):\n",
    "        # If you want to save for each layer, uncomment the following line\n",
    "        # image_preprocessor.save_patches(os.path.join(output_dir, f'layer_{i}_patches'), layer=i)\n",
    "\n",
    "        image_batch, offset_xs, offset_ys = image_preprocessor.get_image_patches(i)\n",
    "        spotter.load_batch(image_batch, offset_xs, offset_ys)\n",
    "        results = spotter.inference_batch()\n",
    "        all_layer_results.extend(results)\n",
    "\n",
    "        #all_layer_offset_xs.extend(offset_xs)\n",
    "        #all_layer_offset_ys.extend(offset_ys)\n",
    "\n",
    "        if save_each_layer == True:\n",
    "            vis.draw(results).save(os.path.join(output_dir, f'combined_tagged_{i}.png'))\n",
    "            vis.save_json(results, os.path.join(output_dir, f'combined_tagged_{i}.json'))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    vis.draw(all_layer_results).save(os.path.join(output_dir, f'combined_tagged_all_layers.png'))\n",
    "    vis.save_json(all_layer_results, os.path.join(output_dir, f'combined_tagged_all_layers.json'))\n",
    "\n",
    "# Run crop on all maps\n",
    "for map_data_subfolder in next(os.walk(map_data_topfolder))[1]:\n",
    "    jpeg_list = glob.glob(map_data_topfolder + '/' + map_data_subfolder + '/*.jpeg')\n",
    "    if len(jpeg_list) != 1:\n",
    "        print(map_data_subfolder + \" failed. Please ensure there is exactly 1 file with extension .jpeg in the folder.\")\n",
    "    else:\n",
    "        map_image = jpeg_list[0].split(\"\\\\\")[1]\n",
    "        if map_data_subfolder in ['1846_vandevelde', '1874_saunders', '1845_kiepert']: # '1858_vandevelde', '1874_saunders', '1845_kiepert']: #,,]: #'1858_vandevelde', '1847_tobler', '1845_kiepert'\n",
    "            img_path = map_data_topfolder + '/' + map_data_subfolder + \"/\" + map_image\n",
    "            map_name = os.path.basename(img_path).split('.')[0] # get the map name without extension\n",
    "            output_dir = os.path.join(map_strec_topfolder, map_name)\n",
    "            if not os.path.isdir(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            pyramid_scan(img_path, output_dir, save_each_layer=False)\n",
    "            logging.info('Done cropping %s' %img_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Label Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import json \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "import Clustering\n",
    "import TextRectify\n",
    "import TextAmalgamate\n",
    "import ExtractHandling\n",
    "import json\n",
    "import pickle\n",
    "import SpotterWrapper\n",
    "import Grouping\n",
    "import BezierSplineMetric\n",
    "import FontSimilarity\n",
    "\n",
    "importlib.reload(SpotterWrapper)\n",
    "importlib.reload(Grouping)\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(TextRectify)\n",
    "importlib.reload(TextAmalgamate)\n",
    "importlib.reload(ExtractHandling)\n",
    "importlib.reload(BezierSplineMetric)\n",
    "importlib.reload(FontSimilarity)\n",
    "\n",
    "\n",
    "map_name_in_strec = 'kiepert_1845'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Subword Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_cluster_pre_merge = True\n",
    "\n",
    "with open(f'processed/strec/{map_name_in_strec}/combined_tagged_all_layers.json', 'r', encoding='utf-8') as f:\n",
    "\n",
    "    clustered = Clustering.cluster_polygons(json.load(f))\n",
    "\n",
    "    # visualize clusters\n",
    "    #image = Clustering.visualize_polygons(clustered, 'processed/strec/kiepert_1845/raw.jpeg')\n",
    "    #image.save('processed/strec/kiepert_1845/combined_tagged_all_layers_clustering.png')\n",
    "\n",
    "for label, cluster in clustered.items():\n",
    "    texts = []\n",
    "    scores = []\n",
    "    for polygon in cluster:\n",
    "        texts.append(polygon['text'])\n",
    "        scores.append(polygon['score'])\n",
    "\n",
    "    rectifier = TextRectify.TextRectifier(0.95, 0.5, 10, True, True)\n",
    "\n",
    "    rectifier.feed_data(texts, scores)\n",
    "\n",
    "    rectifier.fit()\n",
    "\n",
    "    rectified, mask = rectifier.get_rectified_text()\n",
    "\n",
    "    if rectified is None:\n",
    "        rectified = max(texts, key=len)\n",
    "\n",
    "    for i in range(len(cluster)):\n",
    "        cluster[i]['text'] = rectified[i]\n",
    "        cluster[i]['keep'] = mask[i]\n",
    "\n",
    "image = Clustering.visualize_polygons(clustered, f'processed/strec/{map_name_in_strec}/raw.jpeg')\n",
    "image.save(f'processed/strec/{map_name_in_strec}/combined_tagged_all_layers_rectified.png')\n",
    "\n",
    "polygon_x = {}\n",
    "polygon_y = {}\n",
    "texts = {}\n",
    "scores = {}\n",
    "i = 0\n",
    "for label, cluster in clustered.items():\n",
    "    for polygon in cluster:\n",
    "        if do_cluster_pre_merge:\n",
    "            if polygon['keep']:\n",
    "                polygon_x[str(i)] = polygon['polygon_x']\n",
    "                polygon_y[str(i)] = polygon['polygon_y']\n",
    "                texts[str(i)] = polygon['text']\n",
    "                scores[str(i)] = polygon['score']\n",
    "                i += 1\n",
    "        else:\n",
    "            polygon_x[str(i)] = polygon['polygon_x']\n",
    "            polygon_y[str(i)] = polygon['polygon_y']\n",
    "            texts[str(i)] = polygon['text']\n",
    "            scores[str(i)] = polygon['score']\n",
    "            i += 1\n",
    "\n",
    "json_data = {'polygon_x': polygon_x, 'polygon_y': polygon_y, 'text': texts, 'score': scores}\n",
    "\n",
    "with open(f'processed/strec/{map_name_in_strec}/combined_tagged_all_layers_rectified_premerge.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Nested Word Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947 labels.\n",
      "875 labels.\n",
      "854 labels.\n",
      "852 labels.\n",
      "851 labels.\n",
      "Amalgamation completed with 851 labels.\n"
     ]
    }
   ],
   "source": [
    "# Amalgamation stage - assumes there exists \"combined_tagged_all_layers_rectified_premerge.json\" in map_name_in_strec processed folder.\n",
    "df = ExtractHandling.prepare_labels_for_amalgamation(map_name_in_strec)\n",
    "df = TextAmalgamate.amalgamate_labels_wrapper(df, 0.75, .5)\n",
    "\n",
    "# Save amalgamated labels\n",
    "with open(f'processed/strec/{map_name_in_strec}/deduplicated_flattened_labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Multi-Word Sequence Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_name_in_strec = \"vandevelde_1846\"\n",
    "\n",
    "import pickle\n",
    "df = pickle.load(open('processed/strec/' + map_name_in_strec + '/deduplicated_flattened_labels.pickle', 'rb'))\n",
    "\n",
    "df['polygons'] = df['labels'].apply(lambda x: x[0])\n",
    "df['texts'] = df['labels'].apply(lambda x: x[1])\n",
    "\n",
    "# Uncomment to draw splines later\n",
    "## BezierSplineMetric.draw_splines(map_name_in_strec, polygons, texts, PCA_features, all_splines)\n",
    "\n",
    "# reset index so list-based operations match df index\n",
    "df = df.reset_index(drop=True).copy()\n",
    "\n",
    "# pca for principal directions\n",
    "df['PCA_features'] = Grouping.calc_PCA_feats(df['polygons'], do_separation=True, enhance_coords=True)\n",
    "\n",
    "# find neighbors for spline metric consideration\n",
    "df = BezierSplineMetric.calc_neighbours(df, radius_multiplier = 40)\n",
    "\n",
    "# calculate spline metric between identified neighbors\n",
    "df = BezierSplineMetric.spline_metric(df)\n",
    "\n",
    "# Drop PCA_features - no longer needed\n",
    "df.drop('PCA_features', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854\n",
      "854 labels.\n",
      "1 13\n",
      "2 79\n",
      "8 851\n",
      "9 127\n",
      "12 837\n",
      "19 80\n",
      "20 497\n",
      "22 124\n",
      "23 33\n",
      "24 785\n",
      "27 463\n",
      "29 819\n",
      "32 69\n",
      "36 500\n",
      "37 457\n",
      "43 492\n",
      "46 122\n",
      "56 508\n",
      "60 62\n",
      "70 81\n",
      "71 470\n",
      "82 454\n",
      "84 711\n",
      "91 129\n",
      "92 165\n",
      "94 133\n",
      "96 564\n",
      "99 102\n",
      "107 115\n",
      "111 117\n",
      "131 527\n",
      "136 151\n",
      "137 155\n",
      "139 145\n",
      "142 169\n",
      "144 170\n",
      "147 166\n",
      "148 162\n",
      "149 157\n",
      "150 787\n",
      "152 171\n",
      "156 180\n",
      "164 838\n",
      "168 185\n",
      "174 179\n",
      "177 575\n",
      "181 821\n",
      "186 617\n",
      "188 201\n",
      "190 535\n",
      "191 252\n",
      "202 241\n",
      "203 226\n",
      "205 255\n",
      "207 227\n",
      "208 846\n",
      "209 235\n",
      "210 811\n",
      "212 220\n",
      "214 624\n",
      "222 223\n",
      "224 249\n",
      "232 791\n",
      "237 246\n",
      "239 253\n",
      "244 251\n",
      "248 634\n",
      "257 636\n",
      "259 263\n",
      "260 765\n",
      "261 276\n",
      "262 281\n",
      "266 268\n",
      "267 563\n",
      "274 325\n",
      "280 282\n",
      "284 287\n",
      "285 639\n",
      "289 658\n",
      "290 297\n",
      "291 312\n",
      "292 411\n",
      "293 307\n",
      "294 421\n",
      "295 329\n",
      "298 311\n",
      "300 799\n",
      "301 442\n",
      "302 323\n",
      "304 413\n",
      "306 576\n",
      "308 580\n",
      "309 739\n",
      "310 405\n",
      "313 664\n",
      "315 797\n",
      "316 317\n",
      "319 321\n",
      "326 653\n",
      "332 334\n",
      "333 336\n",
      "335 351\n",
      "337 390\n",
      "338 367\n",
      "339 802\n",
      "340 369\n",
      "343 356\n",
      "345 804\n",
      "352 364\n",
      "357 669\n",
      "358 683\n",
      "360 773\n",
      "363 805\n",
      "366 375\n",
      "371 384\n",
      "376 379\n",
      "378 803\n",
      "385 389\n",
      "388 809\n",
      "393 708\n",
      "394 404\n",
      "396 402\n",
      "397 741\n",
      "398 432\n",
      "401 748\n",
      "403 685\n",
      "406 435\n",
      "407 813\n",
      "408 644\n",
      "409 424\n",
      "414 701\n",
      "415 812\n",
      "423 429\n",
      "425 839\n",
      "434 651\n",
      "447 462\n",
      "448 461\n",
      "452 464\n",
      "453 458\n",
      "477 712\n",
      "479 484\n",
      "480 490\n",
      "499 506\n",
      "510 512\n",
      "515 516\n",
      "517 525\n",
      "536 539\n",
      "537 542\n",
      "551 552\n",
      "553 558\n",
      "567 633\n",
      "568 569\n",
      "577 583\n",
      "582 585\n",
      "597 598\n",
      "612 616\n",
      "618 626\n",
      "643 662\n",
      "646 647\n",
      "648 652\n",
      "650 849\n",
      "655 665\n",
      "660 663\n",
      "673 834\n",
      "674 675\n",
      "676 679\n",
      "678 680\n",
      "686 687\n",
      "697 801\n",
      "734 744\n",
      "776 852\n",
      "806 807\n",
      "682 labels.\n",
      "28 992\n",
      "38 858\n",
      "42 860\n",
      "52 875\n",
      "53 869\n",
      "59 868\n",
      "73 871\n",
      "75 857\n",
      "77 872\n",
      "86 870\n",
      "88 861\n",
      "89 997\n",
      "93 879\n",
      "101 877\n",
      "118 855\n",
      "119 865\n",
      "126 883\n",
      "134 859\n",
      "163 898\n",
      "167 886\n",
      "173 896\n",
      "178 996\n",
      "183 863\n",
      "198 909\n",
      "215 911\n",
      "221 902\n",
      "233 917\n",
      "238 916\n",
      "256 949\n",
      "265 922\n",
      "269 948\n",
      "272 931\n",
      "273 926\n",
      "303 938\n",
      "318 945\n",
      "320 933\n",
      "322 936\n",
      "327 930\n",
      "328 940\n",
      "341 956\n",
      "355 961\n",
      "359 959\n",
      "362 960\n",
      "368 965\n",
      "374 957\n",
      "383 968\n",
      "400 975\n",
      "410 935\n",
      "419 937\n",
      "431 974\n",
      "433 1014\n",
      "437 942\n",
      "438 939\n",
      "439 952\n",
      "440 972\n",
      "441 934\n",
      "443 947\n",
      "465 864\n",
      "471 991\n",
      "483 873\n",
      "485 994\n",
      "487 995\n",
      "514 866\n",
      "518 998\n",
      "526 999\n",
      "528 884\n",
      "534 881\n",
      "538 918\n",
      "540 1000\n",
      "543 907\n",
      "544 908\n",
      "546 910\n",
      "555 928\n",
      "559 1002\n",
      "570 1004\n",
      "572 1005\n",
      "573 880\n",
      "581 944\n",
      "584 889\n",
      "586 1006\n",
      "587 951\n",
      "588 878\n",
      "596 1011\n",
      "600 905\n",
      "602 904\n",
      "608 914\n",
      "610 929\n",
      "613 1009\n",
      "615 906\n",
      "620 901\n",
      "622 1010\n",
      "637 921\n",
      "640 913\n",
      "641 912\n",
      "654 986\n",
      "656 982\n",
      "657 987\n",
      "666 941\n",
      "670 963\n",
      "677 1018\n",
      "681 979\n",
      "682 953\n",
      "684 962\n",
      "688 969\n",
      "691 966\n",
      "696 919\n",
      "702 943\n",
      "703 985\n",
      "704 984\n",
      "707 955\n",
      "720 899\n",
      "728 1003\n",
      "731 1008\n",
      "742 1022\n",
      "743 976\n",
      "764 924\n",
      "766 923\n",
      "789 891\n",
      "800 1021\n",
      "808 1023\n",
      "820 862\n",
      "828 915\n",
      "840 946\n",
      "848 885\n",
      "558 labels.\n",
      "40 1027\n",
      "63 1029\n",
      "76 1028\n",
      "78 1026\n",
      "87 1034\n",
      "103 1035\n",
      "108 1037\n",
      "121 1040\n",
      "130 1032\n",
      "135 1042\n",
      "243 1049\n",
      "254 1050\n",
      "324 1061\n",
      "372 1068\n",
      "377 1069\n",
      "381 1067\n",
      "387 1071\n",
      "444 1072\n",
      "469 1083\n",
      "475 1084\n",
      "486 1030\n",
      "488 1085\n",
      "489 1086\n",
      "502 1048\n",
      "520 1088\n",
      "529 1041\n",
      "530 1033\n",
      "532 1091\n",
      "533 1043\n",
      "547 1095\n",
      "562 1058\n",
      "574 1100\n",
      "578 1055\n",
      "579 1054\n",
      "589 1060\n",
      "590 1103\n",
      "591 1105\n",
      "595 1062\n",
      "603 1052\n",
      "621 1114\n",
      "625 1113\n",
      "635 1101\n",
      "645 1059\n",
      "649 1108\n",
      "659 1077\n",
      "667 1064\n",
      "668 1056\n",
      "692 1130\n",
      "698 1075\n",
      "706 1125\n",
      "723 1053\n",
      "732 1096\n",
      "733 1051\n",
      "736 1116\n",
      "745 1115\n",
      "746 1139\n",
      "749 1047\n",
      "754 1136\n",
      "757 1099\n",
      "760 1097\n",
      "763 1137\n",
      "767 1142\n",
      "778 1057\n",
      "784 1090\n",
      "793 1111\n",
      "830 1140\n",
      "833 1145\n",
      "841 1127\n",
      "490 labels.\n",
      "66 1150\n",
      "112 1156\n",
      "331 1164\n",
      "467 1154\n",
      "474 1151\n",
      "478 1159\n",
      "491 1155\n",
      "493 1157\n",
      "494 1171\n",
      "496 1152\n",
      "498 1158\n",
      "523 1168\n",
      "592 1184\n",
      "594 1162\n",
      "604 1161\n",
      "631 1167\n",
      "661 1187\n",
      "672 1165\n",
      "689 1166\n",
      "695 1197\n",
      "705 1163\n",
      "713 1172\n",
      "729 1191\n",
      "730 1183\n",
      "738 1181\n",
      "761 1202\n",
      "768 1211\n",
      "770 1205\n",
      "771 1204\n",
      "772 1189\n",
      "780 1178\n",
      "781 1173\n",
      "792 1209\n",
      "796 1203\n",
      "822 1214\n",
      "823 1179\n",
      "829 1208\n",
      "831 1198\n",
      "452 labels.\n",
      "83 1218\n",
      "184 1219\n",
      "472 1221\n",
      "481 1223\n",
      "503 1224\n",
      "507 1228\n",
      "690 1236\n",
      "700 1237\n",
      "756 1240\n",
      "769 1233\n",
      "779 1245\n",
      "782 1229\n",
      "794 1243\n",
      "795 1244\n",
      "810 1220\n",
      "437 labels.\n",
      "104 1256\n",
      "482 1259\n",
      "501 1257\n",
      "750 1258\n",
      "759 1264\n",
      "818 1260\n",
      "827 1269\n",
      "430 labels.\n",
      "450 1271\n",
      "798 1275\n",
      "843 1277\n",
      "427 labels.\n",
      "786 1278\n",
      "850 1280\n",
      "425 labels.\n",
      "842 1281\n",
      "424 labels.\n",
      "Sequence Recovery completed with 424 labels.\n",
      "424\n",
      "424\n"
     ]
    }
   ],
   "source": [
    "def combine_labels(label1_row, label2_row):\n",
    "\n",
    "    poly1 = label1_row['labels'][0]\n",
    "    text1 = label1_row['labels'][1]\n",
    "    poly2 = label2_row['labels'][0]\n",
    "    text2 = label2_row['labels'][1]\n",
    "    scores1 = label1_row['scores']\n",
    "    scores2 = label2_row['scores']\n",
    "    neighbours1 = label1_row['neighbours']\n",
    "    neighbours2 = label2_row['neighbours']\n",
    "    if neighbours1 is None:\n",
    "        neighbours1 = []\n",
    "    if neighbours2 is None:\n",
    "        neighbours2 = []\n",
    "\n",
    "    poly_new = poly1.union(poly2)\n",
    "\n",
    "    leftmost_poly = [poly1, poly2].index(min([poly1, poly2], key=lambda shape: shape.bounds[0]))\n",
    "    if leftmost_poly == 0:\n",
    "        text_new = text1 + \" \" + text2\n",
    "    else:\n",
    "        text_new = text2 + \" \" + text1\n",
    "\n",
    "    neighbours_new = list(set(neighbours1 + neighbours2))\n",
    "\n",
    "    scores_new = {key: min(scores1.get(key, float('inf')), scores2.get(key, float('inf'))) for key in set(scores1) | set(scores2)}\n",
    "\n",
    "    return [(poly_new, text_new), poly_new, text_new, neighbours_new, scores_new]\n",
    "\n",
    "def recover_sequence(df, R, to_combine):\n",
    "    for pair in to_combine:\n",
    "        if pair[0] in df.index and pair[1] in df.index:\n",
    "            new_label = combine_labels(df.loc[pair[0]], df.loc[pair[1]])\n",
    "            new_label_index = int(df.index[-1]) + 1\n",
    "            df.loc[new_label_index] = new_label\n",
    "            df = df.drop([pair[0]]).copy()\n",
    "            df = df.drop([pair[1]]).copy()\n",
    "            try:\n",
    "                R.pop(pair[0])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                R.pop(pair[1])\n",
    "            except:\n",
    "                pass\n",
    "        else: # one of the polygons has already been recovered into a sequence so no combination can no longer occur\n",
    "            pass\n",
    "    return df, R\n",
    "\n",
    "def update_R_matrix(df, font_threshold, bezier_threshold, R = None):\n",
    "    if R == None:\n",
    "        R = {}\n",
    "    to_combine = []\n",
    "    for i, j in combinations(df.index, 2):\n",
    "        if i not in R.keys():\n",
    "            R[i] = {}\n",
    "        if j in R[i].keys():\n",
    "            pass\n",
    "        else:\n",
    "            font_score = 1 #FontSimilarity.font_sim(crop1, crop2)\n",
    "            spline_distance_score = BezierSplineMetric.get_distance_metric(df, i, j, infinitely_large_as=10000000)\n",
    "            R[i][j] = (font_score, spline_distance_score)\n",
    "            if font_score > font_threshold and spline_distance_score < bezier_threshold:\n",
    "                to_combine.append((i,j))\n",
    "    return R, to_combine\n",
    "\n",
    "def sl_sequence_recovery_wrapper(df, font_threshold, bezier_threshold):\n",
    "\n",
    "    pre_seqrec = 0\n",
    "    post_seqrec = len(df)\n",
    "    R = None\n",
    "\n",
    "    while pre_seqrec - post_seqrec != 0:\n",
    "        pre_seqrec = post_seqrec\n",
    "\n",
    "        # map it to comparison matrix, find candidates for sequences\n",
    "        R, to_combine = update_R_matrix(df, font_threshold, bezier_threshold, R)\n",
    "        print(str(pre_seqrec) + \" labels.\")\n",
    "\n",
    "        # recover sequences based on candidates\n",
    "        df, R = recover_sequence(df, R, to_combine)\n",
    "        post_seqrec = len(df)\n",
    "\n",
    "    print(\"Sequence Recovery completed with \" + str(pre_seqrec) + \" labels.\")\n",
    "    return df\n",
    "\n",
    "df = sl_sequence_recovery_wrapper(df, 0, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels        (MULTIPOLYGON (((2773.2839355469 969.486633300...\n",
       "polygons      MULTIPOLYGON (((2773.2839355469 969.4866333008...\n",
       "texts                                               naby sammil\n",
       "neighbours                                                 None\n",
       "scores        {1: 0.1893620868648559, 776: 0.061774939910144...\n",
       "Name: 851, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[851]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "crop1 = Grouping.polygon_crop(df.iloc[1]['polygons'], Image.open(\"processed/strec/\" + map_name_in_strec + \"/raw.jpeg\"))\n",
    "crop2 = Grouping.polygon_crop(df.iloc[2]['polygons'], Image.open(\"processed/strec/\" + map_name_in_strec + \"/raw.jpeg\"))\n",
    "FontSimilarity.font_sim(crop1, crop2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative process for sequence recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amalgamation stage - assumes there exists \"combined_tagged_all_layers_rectified_premerge.json\" in map_name_in_strec processed folder.\n",
    "df = ?.prepare_labels_for_single_line_sequence_recovery(df)\n",
    "df = ?.single_line_sequence_recovery_wrapper(df, eps11, eps12, eps13)\n",
    "df = ?.multi_line_sequence_recovery_wrapper(df, eps21, eps22, eps23, eps24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import scipy\n",
    "import numpy as np\n",
    "import importlib \n",
    "\n",
    "import Evaluation\n",
    "importlib.reload(Evaluation)\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: Isolate crops to be used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_crop(map_name_in_strec, raw_or_spotter, left_x, right_x, top_y, bottom_y):\n",
    "    if raw_or_spotter == \"raw\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/raw.jpeg') \n",
    "    elif raw_or_spotter == \"spotter_0\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_0.png')\n",
    "    elif raw_or_spotter == \"spotter_1\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_1.png')\n",
    "    elif raw_or_spotter == \"spotter_2\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_2.png')\n",
    "    elif raw_or_spotter == \"all\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_all_layers.png')\n",
    "    elif raw_or_spotter == \"rectified\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_all_layers_rectified.png')\n",
    "    width, height = map_img.size\n",
    "    print(\"full map is \" + str(width) + \" pixels wide by \" + str(height) + \" pixels high.\\n displaying crop:\")\n",
    "    display(map_img.crop((left_x, top_y, right_x, bottom_y, )))\n",
    "\n",
    "left_x = 2475\n",
    "right_x = 3550\n",
    "top_y = 4820\n",
    "bottom_y = 5850\n",
    "\n",
    "#visualize_crop(\"kiepert_1845\", \"all\", left_x, right_x, top_y, bottom_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiepert_gt_patch_1 = [2475, 3550, 4820, 5850]\n",
    "saunders_gt_patch_1 = [3150, 4150, 2250, 3250]\n",
    "saunders_gt_patch_2 = [6750, 7750, 2250, 3250]\n",
    "saunders_gt_patch_3 = [5400, 6400, 4500, 5500]\n",
    "saunders_gt_patch_4 = [7650, 8650, 5400, 6400]\n",
    "saunders_gt_patch_5 = [7650, 8650, 3150, 4150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Precision and Recall: IoU after 1:1 Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retaining 49 labels fully inside crop area\n",
      "retaining 43 labels that have alphabetic characters\n",
      "retaining 74 labels fully inside crop area\n",
      "retaining 69 labels that have alphabetic characters\n",
      "retaining 6 labels fully inside crop area\n",
      "retaining 6 labels that have alphabetic characters\n",
      "retaining 13 labels fully inside crop area\n",
      "retaining 11 labels that have alphabetic characters\n",
      "retaining 11 labels fully inside crop area\n",
      "retaining 10 labels that have alphabetic characters\n",
      "retaining 35 labels fully inside crop area\n",
      "retaining 32 labels that have alphabetic characters\n",
      "retaining 47 labels fully inside crop area\n",
      "retaining 46 labels that have alphabetic characters\n",
      "retaining 87 labels fully inside crop area\n",
      "retaining 87 labels that have alphabetic characters\n",
      "retaining 25 labels fully inside crop area\n",
      "retaining 23 labels that have alphabetic characters\n",
      "retaining 65 labels fully inside crop area\n",
      "retaining 60 labels that have alphabetic characters\n",
      "retaining 33 labels fully inside crop area\n",
      "retaining 33 labels that have alphabetic characters\n",
      "retaining 71 labels fully inside crop area\n",
      "retaining 67 labels that have alphabetic characters\n",
      "retaining 49 labels fully inside crop area\n",
      "retaining 43 labels that have alphabetic characters\n",
      "retaining 32 labels fully inside crop area\n",
      "retaining 32 labels that have alphabetic characters\n",
      "retaining 6 labels fully inside crop area\n",
      "retaining 6 labels that have alphabetic characters\n",
      "retaining 7 labels fully inside crop area\n",
      "retaining 6 labels that have alphabetic characters\n",
      "retaining 11 labels fully inside crop area\n",
      "retaining 10 labels that have alphabetic characters\n",
      "retaining 7 labels fully inside crop area\n",
      "retaining 5 labels that have alphabetic characters\n",
      "retaining 47 labels fully inside crop area\n",
      "retaining 46 labels that have alphabetic characters\n",
      "retaining 9 labels fully inside crop area\n",
      "retaining 8 labels that have alphabetic characters\n",
      "retaining 25 labels fully inside crop area\n",
      "retaining 23 labels that have alphabetic characters\n",
      "retaining 0 labels fully inside crop area\n",
      "retaining 33 labels fully inside crop area\n",
      "retaining 33 labels that have alphabetic characters\n",
      "retaining 0 labels fully inside crop area\n"
     ]
    }
   ],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "## Patch-level geographic pairings for non-multiline-toponyms against our pyramid pipeline\n",
    "pyramid_detected_kiepert, num_gt_kiepert, pyramid_IoU_pairs_kiepert = Evaluation.geographic_evaluation(\"kiepert_1845\", \"components\", kiepert_gt_patch_1)\n",
    "pyramid_detected_saunders1, num_gt_saunders1, pyramid_IoU_pairs_saunders1 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", saunders_gt_patch_1)\n",
    "pyramid_detected_saunders2, num_gt_saunders2, pyramid_IoU_pairs_saunders2 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", saunders_gt_patch_2)\n",
    "pyramid_detected_saunders3, num_gt_saunders3, pyramid_IoU_pairs_saunders3 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", saunders_gt_patch_3)\n",
    "pyramid_detected_saunders4, num_gt_saunders4, pyramid_IoU_pairs_saunders4 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", saunders_gt_patch_4)\n",
    "pyramid_detected_saunders5, num_gt_saunders5, pyramid_IoU_pairs_saunders5 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", saunders_gt_patch_5)\n",
    "\n",
    "## Aggregating pyramid numbers and pairs to map-level figures\n",
    "num_gt_kiepert = num_gt_kiepert\n",
    "num_gt_saunders = num_gt_saunders1 + num_gt_saunders2 + num_gt_saunders3 + num_gt_saunders4 + num_gt_saunders5\n",
    "pyramid_detected_kiepert = pyramid_detected_kiepert\n",
    "pyramid_detected_saunders = pyramid_detected_saunders1 + pyramid_detected_saunders2 + pyramid_detected_saunders3 + pyramid_detected_saunders4 + pyramid_detected_saunders5\n",
    "pyramid_IoU_pairs_kiepert = pyramid_IoU_pairs_kiepert\n",
    "pyramid_IoU_pairs_saunders = np.concatenate((pyramid_IoU_pairs_saunders1, pyramid_IoU_pairs_saunders2, pyramid_IoU_pairs_saunders3, pyramid_IoU_pairs_saunders4, pyramid_IoU_pairs_saunders5))\n",
    "\n",
    "## Patch-level geographic pairings for non-multiline-toponyms against baseline\n",
    "baseline_detected_kiepert, num_gt_kiepert, baseline_IoU_pairs_kiepert = Evaluation.geographic_evaluation(\"kiepert_1845\", \"components\", [2475, 3550, 4820, 5850], \"combined_tagged_0.json\")\n",
    "baseline_detected_saunders1, num_gt_saunders1, baseline_IoU_pairs_saunders1 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", [3150, 4150, 2250, 3250], \"combined_tagged_0.json\")\n",
    "baseline_detected_saunders2, num_gt_saunders2, baseline_IoU_pairs_saunders2 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", [6750, 7750, 2250, 3250], \"combined_tagged_0.json\")\n",
    "baseline_detected_saunders3, num_gt_saunders3, baseline_IoU_pairs_saunders3 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", [5400, 6400, 4500, 5500], \"combined_tagged_0.json\")\n",
    "baseline_detected_saunders4, num_gt_saunders4, baseline_IoU_pairs_saunders4 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", [7650, 8650, 5400, 6400], \"combined_tagged_0.json\")\n",
    "baseline_detected_saunders5, num_gt_saunders5, baseline_IoU_pairs_saunders5 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", [7650, 8650, 3150, 4150], \"combined_tagged_0.json\")\n",
    "\n",
    "## Aggregate baseline numbers and pairs to map-level figures\n",
    "baseline_detected_kiepert = baseline_detected_kiepert\n",
    "baseline_detected_saunders = baseline_detected_saunders1 + baseline_detected_saunders2 + baseline_detected_saunders3 + baseline_detected_saunders4 + baseline_detected_saunders5\n",
    "baseline_IoU_pairs_kiepert = baseline_IoU_pairs_kiepert\n",
    "baseline_IoU_pairs_saunders = np.concatenate((baseline_IoU_pairs_saunders1, baseline_IoU_pairs_saunders2, baseline_IoU_pairs_saunders3, baseline_IoU_pairs_saunders4, baseline_IoU_pairs_saunders5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kiepert baseline\n",
      "\n",
      "Avg of Geographic Precision: 0.19116442583187376\n",
      "Avg of Geographic Recall: 0.14226189829348745\n",
      "Avg of Text Precision: 0.08627091726132564\n",
      "Avg of Text Recall: 0.06420161284563769\n",
      "\n",
      "kiepert pyramid\n",
      "\n",
      "Avg of Geographic Precision: 0.31668587247270935\n",
      "Avg of Geographic Recall: 0.5081703535027197\n",
      "Avg of Text Precision: 0.2951939227107476\n",
      "Avg of Text Recall: 0.47368327132654847\n",
      "\n",
      "saunders baseline\n",
      "\n",
      "Avg of Geographic Precision: 0.06051490020519661\n",
      "Avg of Geographic Recall: 0.00974392460931132\n",
      "Avg of Text Precision: 0.08252460898511729\n",
      "Avg of Text Recall: 0.013287860768790072\n",
      "\n",
      "saunders pyramid\n",
      "\n",
      "Avg of Geographic Precision: 0.21679753988318085\n",
      "Avg of Geographic Recall: 0.4721776927964193\n",
      "Avg of Text Precision: 0.191391017192887\n",
      "Avg of Text Recall: 0.4168431476150166\n"
     ]
    }
   ],
   "source": [
    "# Gimme them numbers :)\n",
    "print(\"\\nkiepert baseline\\n\")\n",
    "Evaluation.prec_rec(baseline_IoU_pairs_kiepert, baseline_detected_kiepert, num_gt_kiepert)\n",
    "print(\"\\nkiepert pyramid\\n\")\n",
    "Evaluation.prec_rec(pyramid_IoU_pairs_kiepert, pyramid_detected_kiepert, num_gt_kiepert)\n",
    "print(\"\\nsaunders baseline\\n\")\n",
    "Evaluation.prec_rec(baseline_IoU_pairs_saunders, baseline_detected_saunders, num_gt_saunders)\n",
    "print(\"\\nsaunders pyramid\\n\")\n",
    "Evaluation.prec_rec(pyramid_IoU_pairs_saunders, pyramid_detected_saunders, num_gt_saunders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
