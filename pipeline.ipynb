{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile #pip install Pillow==9.4.0\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import glob\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from ImageCrop import ImagePreprocessor\n",
    "from SpotterWrapper import Spotter, PolygonVisualizer\n",
    "from IPython.display import display\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Specify filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name folders for raw data and processed data\n",
    "map_data_topfolder = 'raw_maps_20231024'\n",
    "map_strec_topfolder = 'processed/strec'\n",
    "\n",
    "for fp in [map_strec_topfolder]:\n",
    "    if not os.path.isdir(fp):\n",
    "        os.makedirs(fp)\n",
    "\n",
    "# IMPORTANT! Locate spotter directory and detectron weights\n",
    "git_clone_location = 'C:/repo/'\n",
    "spotter_directory = git_clone_location + 'mapkurator-spotter/spotter-v2'\n",
    "model_weights = git_clone_location + 'detectron2-master/detectron2/checkpoint/model_v2_en.pth'\n",
    "spotter_config = spotter_directory + '/configs/PALEJUN/Finetune/Rumsey_Polygon_Finetune.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Crop all jpeg maps in (user defined) map_data_topfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_scan(img_path, output_dir, save_each_layer=False):\n",
    "    image = Image.open(img_path)\n",
    "    image_preprocessor = ImagePreprocessor(image, overlapping_tolerance=0.3, num_layers=5, min_patch_resolution=512, max_patch_resolution=4096)\n",
    "    image_preprocessor.process()\n",
    "    print(\"preprocessing done\")\n",
    "    spotter = Spotter(spotter_config, model_weights, confidence_thresh=0.7, draw_thresh=0.85)\n",
    "    all_layer_results = []\n",
    "\n",
    "    base_image_batch, base_offset_xs, base_offset_ys = image_preprocessor.get_image_patches(0)\n",
    "    vis = PolygonVisualizer()\n",
    "    vis.canvas_from_patches(base_image_batch, base_offset_xs, base_offset_ys)\n",
    "\n",
    "    for i in range(image_preprocessor.num_layers):\n",
    "        # If you want to save for each layer, uncomment the following line\n",
    "        # image_preprocessor.save_patches(os.path.join(output_dir, f'layer_{i}_patches'), layer=i)\n",
    "\n",
    "        image_batch, offset_xs, offset_ys = image_preprocessor.get_image_patches(i)\n",
    "        spotter.load_batch(image_batch, offset_xs, offset_ys)\n",
    "        results = spotter.inference_batch()\n",
    "        all_layer_results.extend(results)\n",
    "\n",
    "        #all_layer_offset_xs.extend(offset_xs)\n",
    "        #all_layer_offset_ys.extend(offset_ys)\n",
    "\n",
    "        if save_each_layer == True:\n",
    "            vis.draw(results).save(os.path.join(output_dir, f'combined_tagged_{i}.png'))\n",
    "            vis.save_json(results, os.path.join(output_dir, f'combined_tagged_{i}.json'))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    vis.draw(all_layer_results).save(os.path.join(output_dir, f'combined_tagged_all_layers.png'))\n",
    "    vis.save_json(all_layer_results, os.path.join(output_dir, f'combined_tagged_all_layers.json'))\n",
    "\n",
    "# Run crop on all maps\n",
    "for map_data_subfolder in next(os.walk(map_data_topfolder))[1]:\n",
    "    jpeg_list = glob.glob(map_data_topfolder + '/' + map_data_subfolder + '/*.jpeg')\n",
    "    if len(jpeg_list) != 1:\n",
    "        print(map_data_subfolder + \" failed. Please ensure there is exactly 1 file with extension .jpeg in the folder.\")\n",
    "    else:\n",
    "        map_image = jpeg_list[0].split(\"\\\\\")[1]\n",
    "        if map_data_subfolder in ['1846_vandevelde', '1874_saunders', '1845_kiepert']: # '1858_vandevelde', '1874_saunders', '1845_kiepert']: #,,]: #'1858_vandevelde', '1847_tobler', '1845_kiepert'\n",
    "            img_path = map_data_topfolder + '/' + map_data_subfolder + \"/\" + map_image\n",
    "            map_name = os.path.basename(img_path).split('.')[0] # get the map name without extension\n",
    "            output_dir = os.path.join(map_strec_topfolder, map_name)\n",
    "            if not os.path.isdir(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            pyramid_scan(img_path, output_dir, save_each_layer=False)\n",
    "            logging.info('Done cropping %s' %img_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Label Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "Using device: cuda\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import json \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "import Clustering\n",
    "import TextRectify\n",
    "import TextAmalgamate\n",
    "import ExtractHandling\n",
    "import json\n",
    "import pickle\n",
    "import SpotterWrapper\n",
    "import Grouping\n",
    "import BezierSplineMetric\n",
    "import FontSimilarity\n",
    "import SequenceRecovery\n",
    "import SubwordDeduplication as sd\n",
    "import NestedWordFlattening as nwf\n",
    "import RumseyMetric\n",
    "importlib.reload(SpotterWrapper)\n",
    "importlib.reload(Grouping)\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(TextRectify)\n",
    "importlib.reload(TextAmalgamate)\n",
    "importlib.reload(ExtractHandling)\n",
    "importlib.reload(BezierSplineMetric)\n",
    "importlib.reload(FontSimilarity)\n",
    "importlib.reload(SequenceRecovery)\n",
    "\n",
    "map_name_in_strec = 'vandevelde_1846' # 'kiepert_1845', 'saunders_1874', 'vandevelde_1846'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Subword Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "971 polygons kept.\n"
     ]
    }
   ],
   "source": [
    "sd.subword_deduplication(map_name_in_strec, do_cluster_pre_merge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Nested Word Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_vis(df, map_name_in_strec, suffix):\n",
    "    polygons = [el[0] for el in df['labels']]\n",
    "    texts = [el[1] for el in df['labels']]\n",
    "    vis = SpotterWrapper.PolygonVisualizer()\n",
    "    canvas = Image.open(f'processed/strec/{map_name_in_strec}/raw.jpeg')\n",
    "    vis.canvas_from_image(canvas)\n",
    "    vis.draw_poly(polygons, texts, PCA_feature_list=None, BSplines=None, random_color=True)\n",
    "    vis.save(f'processed/strec/{map_name_in_strec}/testing_{suffix}.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started NWF with 971 labels.\n",
      "Retained 865.\n"
     ]
    }
   ],
   "source": [
    "# Amalgamation stage - assumes there exists \"combined_tagged_all_layers_rectified_premerge.json\" in map_name_in_strec processed folder.\n",
    "df = ExtractHandling.prepare_labels_for_amalgamation(map_name_in_strec)\n",
    "#df_orig = TextAmalgamate.amalgamate_labels_wrapper(df, 0.75, .5)\n",
    "df_new = pd.DataFrame({\"labels\": nwf.nwf_wrapper(df['labels'].tolist(), 0.75, 0.5)})\n",
    "\n",
    "# Save amalgamated labels\n",
    "with open(f'processed/strec/{map_name_in_strec}/deduplicated_flattened_labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_new, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save visualization\n",
    "testing_vis(df_new, map_name_in_strec, \"orig_nwf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Multi-Word Sequence Recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Prepare by calculating spline and font metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('processed/strec/' + map_name_in_strec + '/deduplicated_flattened_labels.pickle', 'rb'))\n",
    "df['polygons'] = df['labels'].apply(lambda x: x[0])\n",
    "df['texts'] = df['labels'].apply(lambda x: x[1])\n",
    "\n",
    "# Uncomment to draw splines later\n",
    "## BezierSplineMetric.draw_splines(map_name_in_strec, polygons, texts, PCA_features, all_splines)\n",
    "\n",
    "# reset index so list-based operations match df index\n",
    "df = df.reset_index(drop=True).copy()\n",
    "\n",
    "# pca for principal directions\n",
    "df['PCA_features'] = Grouping.calc_PCA_feats(df['polygons'], do_separation=True, enhance_coords=True)\n",
    "\n",
    "# find neighbors for spline and font metric consideration\n",
    "df = BezierSplineMetric.calc_neighbours(df, radius_multiplier = 40)\n",
    "\n",
    "# calculate spline metric between identified neighbors\n",
    "df = BezierSplineMetric.spline_metric(df)\n",
    "\n",
    "# calculate font metric between identified neighbors - long due to need to work with images\n",
    "df = FontSimilarity.calc_font_similarities(df, map_name_in_strec)\n",
    "\n",
    "# calculate rumsey metric for combination\n",
    "df = RumseyMetric.calc_rumsey_metric(df)\n",
    "\n",
    "with open(f'processed/strec/{map_name_in_strec}/seq_rec_prepared_labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional - draw splines\n",
    "#df = pickle.load(open(f'processed/strec/{map_name_in_strec}/seq_rec_prepared_labels.pickle', 'rb'))\n",
    "#BezierSplineMetric.draw_splines(map_name_in_strec, df['polygons'].tolist(), df['texts'].tolist(), df['PCA_features'].tolist(), df['all_splines'].explode().dropna().tolist(), spline_metric_threshold = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Iterative Sequence Recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Rumsey's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "865 labels.\n",
      "584 labels.\n",
      "Sequence Recovery completed with 584 labels.\n"
     ]
    }
   ],
   "source": [
    "# load in seq rec prepared\n",
    "df = pickle.load(open(f'processed/strec/{map_name_in_strec}/seq_rec_prepared_labels.pickle', 'rb'))\n",
    "## Drop PCA_features - no longer needed, makes me feel good to discard stuff i don't need\n",
    "df.drop('PCA_features', axis=1, inplace=True)\n",
    "df.drop('all_splines', axis=1, inplace=True)\n",
    "\n",
    "# calculate using rumsey metric\n",
    "df = SequenceRecovery.sl_sequence_recovery_wrapper(df, use_rumsey_metric=True)\n",
    "\n",
    "# map to dataframe\n",
    "new_texts = []\n",
    "new_labels = []\n",
    "for index, row in df.iterrows():\n",
    "    sorted_text = sorted(row['text_list'], key=lambda x: x[0][0])\n",
    "    new_texts.append(\" \".join([_text[1] for _text in sorted_text]))\n",
    "    new_labels.append((row['labels'][0], row['texts']))\n",
    "df['labels'] = new_labels\n",
    "df['texts'] = new_texts\n",
    "\n",
    "# save\n",
    "with open(f'processed/strec/{map_name_in_strec}/fully_processed_labels_rumsey.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Our Method - Old, Issues with Two Recovered Sequences Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "865 labels.\n",
      "604 labels.\n",
      "502 labels.\n",
      "465 labels.\n",
      "450 labels.\n",
      "443 labels.\n",
      "441 labels.\n",
      "Sequence Recovery completed with 441 labels.\n"
     ]
    }
   ],
   "source": [
    "# load in seq rec prepared\n",
    "df = pickle.load(open(f'processed/strec/{map_name_in_strec}/seq_rec_prepared_labels.pickle', 'rb'))\n",
    "## Drop PCA_features - no longer needed, makes me feel good to discard stuff i don't need\n",
    "df.drop('PCA_features', axis=1, inplace=True)\n",
    "df.drop('all_splines', axis=1, inplace=True)\n",
    "\n",
    "# calculate using our metric\n",
    "df = SequenceRecovery.sl_sequence_recovery_wrapper(df, font_threshold=.5, bezier_threshold=1.5, use_rumsey_metric=False)\n",
    "\n",
    "# map to dataframe\n",
    "new_texts = []\n",
    "new_labels = []\n",
    "for index, row in df.iterrows():\n",
    "    sorted_text = sorted(row['text_list'], key=lambda x: x[0][0])\n",
    "    new_texts.append(\" \".join([_text[1] for _text in sorted_text]))\n",
    "    new_labels.append((row['labels'][0], row['texts']))\n",
    "\n",
    "df['labels'] = new_labels\n",
    "df['texts'] = new_texts\n",
    "\n",
    "# save\n",
    "with open(f'processed/strec/{map_name_in_strec}/fully_processed_labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Our Method - New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from igraph import Graph\n",
    "\n",
    "# draw edges between nodes where bezier and font values pass the threshold\n",
    "def calculate_edges(g, font_threshold, bezier_threshold):\n",
    "    node_indices = range(len(g.vs))\n",
    "    all_pairwise_combs = list(combinations(node_indices, 2))\n",
    "    for pair in all_pairwise_combs:\n",
    "        i = pair[0]\n",
    "        j = pair[1]\n",
    "        if i != j and not g.are_connected(i, j) and (j in g.vs[i]['neighbours']):\n",
    "            if g.vs[i]['font_similarities'][j] > font_threshold and g.vs[i]['bezier_costs'][j] < bezier_threshold:\n",
    "                g.add_edge(i, j)\n",
    "    return g\n",
    "\n",
    "def remap_dictionary_of_indices(dict, indices_to_map):\n",
    "    for from_num in indices_to_map:\n",
    "        if from_num in dict.keys():\n",
    "            to_num = indices_to_map[from_num]\n",
    "            dict[to_num] = dict.pop(from_num)\n",
    "    return dict\n",
    "\n",
    "def remap_set_of_indices(set, indices_to_map):\n",
    "    return {indices_to_map[element] if element in indices_to_map.keys() else element for element in set}\n",
    "\n",
    "def update_ind_map(local_dict, global_dict, from_ind, to_ind):\n",
    "    local_dict[from_ind] = to_ind\n",
    "    global_dict[from_ind] = to_ind\n",
    "    for key, value in global_dict.items():\n",
    "        if value == from_ind:\n",
    "            dict[key] = to_ind\n",
    "    return local_dict, global_dict\n",
    "\n",
    "# combine two labels\n",
    "def combine_labels(v1, v2, global_indices_to_map):\n",
    "\n",
    "    v1_ind = v1['index']\n",
    "    v2_ind = v2['index']\n",
    "    index_new = min(v1_ind, v2_ind)\n",
    "    local_indices_to_map = {}\n",
    "    if index_new == v1_ind:\n",
    "        local_indices_to_map, global_indices_to_map = update_ind_map(local_indices_to_map, global_indices_to_map, v2_ind, v1_ind)\n",
    "    else:\n",
    "        local_indices_to_map, global_indices_to_map = update_ind_map(local_indices_to_map, global_indices_to_map, v1_ind, v2_ind)\n",
    "\n",
    "    poly1 = v1['label'][0]\n",
    "    poly2 = v2['label'][0]\n",
    "    poly_new = poly1.union(poly2) # returns multipolygon object with disjoint polygons if polygons are disjoint\n",
    "\n",
    "    text_new = ''\n",
    "\n",
    "    label_new = (poly_new, text_new)\n",
    "\n",
    "    bezier_costs1 = v1['bezier_costs']\n",
    "    bezier_costs2 = v2['bezier_costs']\n",
    "    bezier_costs_new = {key: min(bezier_costs1.get(key, float('inf')), bezier_costs2.get(key, float('inf'))) for key in set(bezier_costs1) | set(bezier_costs2)}\n",
    "    bezier_costs_new = remap_dictionary_of_indices(bezier_costs_new, local_indices_to_map)\n",
    "\n",
    "    font_similarities1 = v1['font_similarities']\n",
    "    font_similarities2 = v2['font_similarities']\n",
    "    font_similarities_new = {key: max(font_similarities1.get(key, 0), font_similarities2.get(key, 0)) for key in set(font_similarities1) | set(font_similarities2)}\n",
    "    font_similarities_new = remap_dictionary_of_indices(font_similarities_new, local_indices_to_map)\n",
    "\n",
    "    neighbours1 = set(v1['neighbours'])\n",
    "    neighbours2 = set(v2['neighbours'])\n",
    "    if neighbours1 is None:\n",
    "        neighbours1 = []\n",
    "    if neighbours2 is None:\n",
    "        neighbours2 = []\n",
    "    neighbours_new = neighbours1.union(neighbours2)\n",
    "    neighbours_new.discard(v1_ind)\n",
    "    neighbours_new.discard(v2_ind)\n",
    "    neighbours_new = list(neighbours_new)\n",
    "\n",
    "    return index_new, label_new, bezier_costs_new, font_similarities_new, neighbours_new, global_indices_to_map\n",
    "\n",
    "# combine two nodes by adding to new graph and using combine_labels() function above for attribute\n",
    "def subgraph_contractor(subgraph, edges_calculated, font_threshold, bezier_threshold, global_indices_to_map):\n",
    "    if edges_calculated:\n",
    "        pass\n",
    "    else:\n",
    "        subgraph = calculate_edges(subgraph, font_threshold, bezier_threshold)\n",
    "    edges = subgraph.get_edgelist()\n",
    "    uncontracted_vertices = {i for i in range(len(subgraph.vs))}\n",
    "    subgraph_new = Graph()\n",
    "    for edge in edges:\n",
    "        if edge[0] in uncontracted_vertices and edge[1] in uncontracted_vertices:\n",
    "            index_new, label_new, bezier_costs_new, font_similarities_new, neighbours_new, global_indices_to_map = combine_labels(subgraph.vs[edge[0]], subgraph.vs[edge[1]], global_indices_to_map)\n",
    "            subgraph_new.add_vertex(index = index_new, label = label_new, bezier_costs = bezier_costs_new, font_similarities = font_similarities_new, neighbours = neighbours_new)\n",
    "            uncontracted_vertices.remove(edge[0])\n",
    "            uncontracted_vertices.remove(edge[1])\n",
    "    for vertex in uncontracted_vertices:\n",
    "        tmp_v = subgraph.vs[vertex]\n",
    "        subgraph_new.add_vertex(index = tmp_v['index'], label = tmp_v['label'], bezier_costs = tmp_v['bezier_costs'], font_similarities = tmp_v['font_similarities'], neighbours = tmp_v['neighbours'])\n",
    "    return subgraph_new, False, global_indices_to_map\n",
    "\n",
    "# wrapper for continued combination until weak connected subgraph cannot be further contracted \n",
    "def subgraph_contractor_wrapper(subgraph, font_threshold, bezier_threshold, global_indices_to_map):\n",
    "    edges_calculated = True\n",
    "    base_len = 0\n",
    "    contracted_len = len(subgraph.vs)\n",
    "    while base_len != contracted_len:\n",
    "        base_len = len(subgraph.vs)\n",
    "        subgraph, edges_calculated, global_indices_to_map = subgraph_contractor(subgraph, edges_calculated, font_threshold, bezier_threshold, global_indices_to_map)\n",
    "        contracted_len = len(subgraph.vs)\n",
    "    return subgraph, global_indices_to_map\n",
    "\n",
    "# prepare subgraphs for flattening base on IoMs\n",
    "def sl_seq_req(indices, labels, bezier_costs, font_similarities, neighbours, font_threshold, bezier_threshold):\n",
    "\n",
    "    # create graph from labels, extract weak connected components (for isolated seq req)\n",
    "    label_dict_for_graph = dict(zip(['index','label', 'font_similarities', 'bezier_costs', 'neighbours'], [indices, labels, font_similarities, bezier_costs, neighbours]))\n",
    "    g = Graph()\n",
    "    g.add_vertices(len(labels),attributes=label_dict_for_graph)\n",
    "    print(g.vs[1]['bezier_costs'])\n",
    "    g = calculate_edges(g, font_threshold, bezier_threshold)\n",
    "    connected_subgraphs = g.decompose()\n",
    "    global_indices_to_map = {}\n",
    "\n",
    "    # seq req weak connected components\n",
    "    for i, subgraph in enumerate(connected_subgraphs, start=1):\n",
    "        if subgraph.vcount() > 1:\n",
    "            subgraph, global_indices_to_map = subgraph_contractor_wrapper(subgraph, font_threshold, bezier_threshold, global_indices_to_map)\n",
    "\n",
    "    print(str(len(connected_subgraphs) + \" labels after curr round of sequence recovery.\"))\n",
    "    iter_indices = []\n",
    "    iter_labels = []\n",
    "    iter_bezier_costs = []\n",
    "    iter_font_similarities = []\n",
    "    iter_neighbours = []\n",
    "    for i, subgraph in enumerate(connected_subgraphs, start=1):\n",
    "        iter_indices.extend([node['index'] for node in subgraph.vs])\n",
    "        iter_labels.extend([node['label'] for node in subgraph.vs])\n",
    "        iter_bezier_costs.extend([remap_dictionary_of_indices(node['bezier_costs'], global_indices_to_map) for node in subgraph.vs])\n",
    "        iter_font_similarities.extend([remap_dictionary_of_indices(node['font_similarities'], global_indices_to_map) for node in subgraph.vs])\n",
    "        iter_neighbours.extend([remap_set_of_indices(node['neighbours'], global_indices_to_map) for node in subgraph.vs])\n",
    "    return iter_labels, iter_bezier_costs, iter_font_similarities, iter_neighbours\n",
    "\n",
    "# wrapper for nwf (apply nwf to weak connected subgraphs until no more connected components manifest) \n",
    "def sl_seq_req_wrapper(labels, bezier_costs, font_similarities, neighbours, font_threshold, bezier_threshold):\n",
    "    print(\"Started SL SR with \" + str(len(labels)) + \" labels.\")\n",
    "    base_len = 0\n",
    "    sreq_len = len(labels)\n",
    "    indices = [i for i in range(len(labels))]\n",
    "    while base_len != sreq_len:\n",
    "        base_len = len(labels)\n",
    "        indices, labels, bezier_scores, font_scores, neighbours = sl_seq_req(indices, labels, bezier_costs, font_similarities, neighbours, font_threshold, bezier_threshold)\n",
    "        sreq_len = len(labels)\n",
    "    print(\"Retained \" + str(len(labels)) + \".\")\n",
    "    return labels, bezier_scores, font_scores, neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started SL SR with 865 labels.\n",
      "{0: 0.3199538701430699, 80: 10.353152165106398, 213: 40.49653968653606, 218: 192.441951782067, 644: 1.0780189356298586, 802: 81.26057037971508, 803: 36.78602254981498, 804: 22.500761619038922, 806: 180.67736955281836, 808: 222.90401705015285, 839: 161.316903212114}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m neighbours \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneighbours\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 11\u001b[0m \u001b[43msl_seq_req_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbezier_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbours\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[56], line 150\u001b[0m, in \u001b[0;36msl_seq_req_wrapper\u001b[1;34m(labels, bezier_costs, font_similarities, neighbours, font_threshold, bezier_threshold)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m base_len \u001b[38;5;241m!=\u001b[39m sreq_len:\n\u001b[0;32m    149\u001b[0m     base_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[1;32m--> 150\u001b[0m     indices, labels, bezier_scores, font_scores, neighbours \u001b[38;5;241m=\u001b[39m \u001b[43msl_seq_req\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbezier_costs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_similarities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbours\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbezier_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     sreq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetained \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labels)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[56], line 128\u001b[0m, in \u001b[0;36msl_seq_req\u001b[1;34m(indices, labels, bezier_costs, font_similarities, neighbours, font_threshold, bezier_threshold)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subgraph\u001b[38;5;241m.\u001b[39mvcount() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    126\u001b[0m         subgraph, global_indices_to_map \u001b[38;5;241m=\u001b[39m subgraph_contractor_wrapper(subgraph, font_threshold, bezier_threshold, global_indices_to_map)\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconnected_subgraphs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m labels after curr round of sequence recovery.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m))\n\u001b[0;32m    129\u001b[0m iter_indices \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    130\u001b[0m iter_labels \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "# load in seq rec prepared\n",
    "df = pickle.load(open(f'processed/strec/{map_name_in_strec}/seq_rec_prepared_labels.pickle', 'rb'))\n",
    "## Drop PCA_features - no longer needed, makes me feel good to discard stuff i don't need\n",
    "df.drop('PCA_features', axis=1, inplace=True)\n",
    "df.drop('all_splines', axis=1, inplace=True)\n",
    "\n",
    "bezier_scores = df['bezier_scores'].tolist()\n",
    "font_scores = df['font_scores'].tolist()\n",
    "neighbours = df['neighbours'].tolist()\n",
    "labels = df['labels'].tolist()\n",
    "sl_seq_req_wrapper(labels, bezier_scores, font_scores, neighbours, 0.5, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import scipy\n",
    "import numpy as np\n",
    "import importlib \n",
    "\n",
    "import Evaluation\n",
    "importlib.reload(Evaluation)\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: Isolate crops to be used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_crop(map_name_in_strec, raw_or_spotter, left_x, right_x, top_y, bottom_y):\n",
    "    if raw_or_spotter == \"raw\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/raw.jpeg') \n",
    "    elif raw_or_spotter == \"spotter_0\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_0.png')\n",
    "    elif raw_or_spotter == \"spotter_1\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_1.png')\n",
    "    elif raw_or_spotter == \"spotter_2\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_2.png')\n",
    "    elif raw_or_spotter == \"all\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_all_layers.png')\n",
    "    elif raw_or_spotter == \"rectified\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_all_layers_rectified.png')\n",
    "    width, height = map_img.size\n",
    "    print(\"full map is \" + str(width) + \" pixels wide by \" + str(height) + \" pixels high.\\n displaying crop:\")\n",
    "    display(map_img.crop((left_x, top_y, right_x, bottom_y, )))\n",
    "\n",
    "left_x = 2475\n",
    "right_x = 3550\n",
    "top_y = 4820\n",
    "bottom_y = 5850\n",
    "\n",
    "#visualize_crop(\"kiepert_1845\", \"all\", left_x, right_x, top_y, bottom_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Precision and Recall: 1:1 Matching on Geometry, then IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kiepert baseline (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.1201465060772278\n",
      "Avg of Text Precision: 0.10528688619228874\n",
      "Avg of Geographic Recall: 0.6131614792906798\n",
      "Avg of Text Recall: 0.5373261778089218\n",
      "\n",
      "kiepert pyramid - subword dedup, nested word flattening (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.23789807623121678\n",
      "Avg of Text Precision: 0.21139102508949265\n",
      "Avg of Geographic Recall: 0.546892129267165\n",
      "Avg of Text Recall: 0.48595637951607507\n",
      "\n",
      "kiepert pyramid - subword dedup, nested word flattening, Rumsey's sequence recovery (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.3005359153606449\n",
      "Avg of Text Precision: 0.26485513144958256\n",
      "Avg of Geographic Recall: 0.4248956044753945\n",
      "Avg of Text Recall: 0.3744503582563064\n",
      "\n",
      "kiepert pyramid - subword dedup, nested word flattening, Our's sequence recovery (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.4885405035688952\n",
      "Avg of Text Precision: 0.5033158403198099\n",
      "Avg of Geographic Recall: 0.6233102976568663\n",
      "Avg of Text Recall: 0.6421615893735506\n",
      "\n",
      "vandevelde baseline (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.10768564771776375\n",
      "Avg of Text Precision: 0.11568854680573216\n",
      "Avg of Geographic Recall: 0.5109340306608791\n",
      "Avg of Text Recall: 0.548905232716559\n",
      "\n",
      "vandevelde pyramid - subword dedup, nested word flattening (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.29380799413381703\n",
      "Avg of Text Precision: 0.29109991325831946\n",
      "Avg of Geographic Recall: 0.5532342017200597\n",
      "Avg of Text Recall: 0.5481349430502399\n",
      "\n",
      "vandevelde pyramid - subword dedup, nested word flattening, Rumsey's sequence recovery (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.3616274748716752\n",
      "Avg of Text Precision: 0.37138339697686806\n",
      "Avg of Geographic Recall: 0.21928474540090942\n",
      "Avg of Text Recall: 0.2252005705072498\n",
      "\n",
      "vandevelde pyramid - subword dedup, nested word flattening, Our's sequence recovery (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.537909329354797\n",
      "Avg of Text Precision: 0.5857931200063767\n",
      "Avg of Geographic Recall: 0.6351908038125794\n",
      "Avg of Text Recall: 0.6917344289437002\n",
      "\n",
      "saunders baseline (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.10692526759221738\n",
      "Avg of Text Precision: 0.09822957271770835\n",
      "Avg of Geographic Recall: 0.5251917555264795\n",
      "Avg of Text Recall: 0.48248054834874393\n",
      "\n",
      "saunders pyramid - subword dedup, nested word flattening (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.2581745272674626\n",
      "Avg of Text Precision: 0.23570070060554776\n",
      "Avg of Geographic Recall: 0.5315357914330112\n",
      "Avg of Text Recall: 0.4852661483055395\n",
      "\n",
      "saunders pyramid - subword dedup, nested word flattening, Rumsey's sequence recovery (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.35177136748552323\n",
      "Avg of Text Precision: 0.2869544764650307\n",
      "Avg of Geographic Recall: 0.42160833014809035\n",
      "Avg of Text Recall: 0.34392337988088234\n",
      "\n",
      "saunders pyramid - subword dedup, nested word flattening, Our's sequence recovery (gt = components)\n",
      "\n",
      "Avg of Geographic Precision: 0.5270308563826411\n",
      "Avg of Text Precision: 0.4950248814300967\n",
      "Avg of Geographic Recall: 0.461151999334811\n",
      "Avg of Text Recall: 0.4331467712513346\n"
     ]
    }
   ],
   "source": [
    "kiepert_gt_patches = [[1750, 3750, 4775, 6200], [2250, 4050, 6050, 7500]] #,[2475, 3550, 4820, 5850]]\n",
    "saunders_gt_patches = [[2350, 3850, 1750, 3250], [6450, 7500, 2200, 3250], [5400, 6400, 4500, 5500], [7650, 8650, 5400, 6400], [7650, 8650, 3150, 4150]] #\n",
    "vandevelde_gt_patches = [[2850, 5250, 1450, 3850]]\n",
    "\n",
    "# Gimme them numbers :)\n",
    "\n",
    "kname = \"kiepert_1845\"\n",
    "vname = \"vandevelde_1846\"\n",
    "sname = \"saunders_1874\"\n",
    "\n",
    "multiline_handling = \"components\" # \"largest\" for multiline gt\n",
    "\n",
    "print(\"\\nkiepert baseline (gt = \" + multiline_handling + \")\\n\")\n",
    "kbase_geo_prec, kbase_text_prec, kbase_geo_rec, kbase_text_rec, kbase_IoU_pairs, kbase_num_detected, kbase_num_gt = Evaluation.prec_rec(kname, multiline_handling, kiepert_gt_patches, \"methods_0\")\n",
    "print(\"\\nkiepert pyramid - subword dedup, nested word flattening (gt = \" + multiline_handling + \")\\n\")\n",
    "k12_geo_prec, k12_text_prec, k12_geo_rec, k12_text_rec, k12_IoU_pairs, k12_num_detected, k12_num_gt = Evaluation.prec_rec(kname, multiline_handling, kiepert_gt_patches, \"methods_1_2\")\n",
    "print(\"\\nkiepert pyramid - subword dedup, nested word flattening, Rumsey's sequence recovery (gt = \" + multiline_handling + \")\\n\")\n",
    "k123_geo_prec, k123_text_prec, k123_geo_rec, k123_text_rec, k123_IoU_pairs, k123_num_detected, k123_num_gt = Evaluation.prec_rec(kname, multiline_handling, kiepert_gt_patches, \"methods_1_2_r\")\n",
    "print(\"\\nkiepert pyramid - subword dedup, nested word flattening, Our's sequence recovery (gt = \" + multiline_handling + \")\\n\")\n",
    "k123_geo_prec, k123_text_prec, k123_geo_rec, k123_text_rec, k123_IoU_pairs, k123_num_detected, k123_num_gt = Evaluation.prec_rec(kname, multiline_handling, kiepert_gt_patches, \"methods_1_2_3\")\n",
    "\n",
    "print(\"\\nvandevelde baseline (gt = \" + multiline_handling + \")\\n\")\n",
    "vbase_geo_prec, vbase_text_prec, vbase_geo_rec, vbase_text_rec, vbase_IoU_pairs, vbase_num_detected, vbase_num_gt = Evaluation.prec_rec(vname, multiline_handling, vandevelde_gt_patches, \"methods_0\")\n",
    "print(\"\\nvandevelde pyramid - subword dedup, nested word flattening (gt = \" + multiline_handling + \")\\n\")\n",
    "v12_geo_prec, v12_text_prec, v12_geo_rec, v12_text_rec, v12_IoU_pairs, v12_num_detected, v12_num_gt = Evaluation.prec_rec(vname, multiline_handling, vandevelde_gt_patches, \"methods_1_2\")\n",
    "print(\"\\nvandevelde pyramid - subword dedup, nested word flattening, Rumsey's sequence recovery (gt = \" + multiline_handling + \")\\n\")\n",
    "v123_geo_prec, v123_text_prec, v123_geo_rec, v123_text_rec, v123_IoU_pairs, v123_num_detected, v123_num_gt = Evaluation.prec_rec(vname, multiline_handling, vandevelde_gt_patches, \"methods_1_2_r\")\n",
    "print(\"\\nvandevelde pyramid - subword dedup, nested word flattening, Our's sequence recovery (gt = \" + multiline_handling + \")\\n\")\n",
    "v123_geo_prec, v123_text_prec, v123_geo_rec, v123_text_rec, v123_IoU_pairs, v123_num_detected, v123_num_gt = Evaluation.prec_rec(vname, multiline_handling, vandevelde_gt_patches, \"methods_1_2_3\")\n",
    "\n",
    "print(\"\\nsaunders baseline (gt = \" + multiline_handling + \")\\n\")\n",
    "sbase_geo_prec, sbase_text_prec, sbase_geo_rec, sbase_text_rec, sbase_IoU_pairs, sbase_num_detected, sbase_num_gt = Evaluation.prec_rec(sname, multiline_handling, saunders_gt_patches, \"methods_0\")\n",
    "print(\"\\nsaunders pyramid - subword dedup, nested word flattening (gt = \" + multiline_handling + \")\\n\")\n",
    "s12_geo_prec, s12_text_prec, s12_geo_rec, s12_text_rec, s12_IoU_pairs, s12_num_detected, s12_num_gt = Evaluation.prec_rec(sname, multiline_handling, saunders_gt_patches, \"methods_1_2\")\n",
    "print(\"\\nsaunders pyramid - subword dedup, nested word flattening, Rumsey's sequence recovery (gt = \" + multiline_handling + \")\\n\")\n",
    "s123_geo_prec, s123_text_prec, s123_geo_rec, s123_text_rec, s123_IoU_pairs, s123_num_detected, s123_num_gt = Evaluation.prec_rec(sname, multiline_handling, saunders_gt_patches, \"methods_1_2_r\")\n",
    "print(\"\\nsaunders pyramid - subword dedup, nested word flattening, Our's sequence recovery (gt = \" + multiline_handling + \")\\n\")\n",
    "s123_geo_prec, s123_text_prec, s123_geo_rec, s123_text_rec, s123_IoU_pairs, s123_num_detected, s123_num_gt = Evaluation.prec_rec(sname, multiline_handling, saunders_gt_patches, \"methods_1_2_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Plot and Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely as sh\n",
    "save_map_name_in_strec = 'vandevelde_1846' # 'kiepert_1845', 'saunders_1874', 'vandevelde_1846'\n",
    "\n",
    "# Plot the final image\n",
    "Evaluation.plot_recovered_seq(save_map_name_in_strec, \"methods_1_2_3\")\n",
    "Evaluation.plot_recovered_seq(save_map_name_in_strec, 'methods_1_2_r', '(rumsey)')\n",
    "\n",
    "# Save final results into a json file\n",
    "spotter_labels_full = ExtractHandling.load_processed_labels(save_map_name_in_strec, \"methods_1_2_3\")\n",
    "polygon_list = spotter_labels_full['label_polygons'].tolist()\n",
    "text_list = spotter_labels_full['annotation'].tolist()\n",
    "\n",
    "polygons_json = []\n",
    "for poly, text in zip(polygon_list, text_list):\n",
    "    dict = {'polygon_x': [], 'polygon_y': [], 'text': text}\n",
    "    if isinstance(poly, sh.geometry.polygon.Polygon):\n",
    "        dict['polygon_x'] = list(poly.exterior.coords.xy[0])\n",
    "        dict['polygon_y'] = list(poly.exterior.coords.xy[1])\n",
    "            \n",
    "    elif isinstance(poly, sh.geometry.multipolygon.MultiPolygon):\n",
    "        for p in poly.geoms: # kaede added .geoms - package version differences\n",
    "            polygon_x = p.exterior.coords.xy[0]\n",
    "            polygon_y = p.exterior.coords.xy[1]\n",
    "\n",
    "            for x, y in zip(polygon_x, polygon_y):\n",
    "                dict['polygon_x'].append(x)\n",
    "                dict['polygon_y'].append(y)\n",
    "\n",
    "    polygons_json.append(dict)\n",
    "\n",
    "with open(f'processed/strec/{save_map_name_in_strec}/final.json', 'w') as f:\n",
    "    json.dump(polygons_json, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
