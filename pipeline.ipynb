{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile #pip install Pillow==9.4.0\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import glob\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from ImageCrop import ImagePreprocessor\n",
    "from SpotterWrapper import Spotter, PolygonVisualizer\n",
    "from IPython.display import display\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Specify filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name folders for raw data and processed data\n",
    "map_data_topfolder = 'raw_maps_20231024'\n",
    "map_strec_topfolder = 'processed/strec'\n",
    "\n",
    "for fp in [map_strec_topfolder]:\n",
    "    if not os.path.isdir(fp):\n",
    "        os.makedirs(fp)\n",
    "\n",
    "# IMPORTANT! Locate spotter directory and detectron weights\n",
    "git_clone_location = 'C:/repo/'\n",
    "spotter_directory = git_clone_location + 'mapkurator-spotter/spotter-v2'\n",
    "model_weights = git_clone_location + 'detectron2-master/detectron2/checkpoint/model_v2_en.pth'\n",
    "spotter_config = spotter_directory + '/configs/PALEJUN/Finetune/Rumsey_Polygon_Finetune.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Crop all jpeg maps in (user defined) map_data_topfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_scan(img_path, output_dir, save_each_layer=False):\n",
    "    image = Image.open(img_path)\n",
    "    image_preprocessor = ImagePreprocessor(image, overlapping_tolerance=0.3, num_layers=5, min_patch_resolution=512, max_patch_resolution=4096)\n",
    "    image_preprocessor.process()\n",
    "    print(\"preprocessing done\")\n",
    "    spotter = Spotter(spotter_config, model_weights, confidence_thresh=0.8, draw_thresh=0.85)\n",
    "    all_layer_results = []\n",
    "\n",
    "    base_image_batch, base_offset_xs, base_offset_ys = image_preprocessor.get_image_patches(0)\n",
    "    vis = PolygonVisualizer()\n",
    "    vis.canvas_from_patches(base_image_batch, base_offset_xs, base_offset_ys)\n",
    "\n",
    "    for i in range(image_preprocessor.num_layers):\n",
    "        # If you want to save for each layer, uncomment the following line\n",
    "        # image_preprocessor.save_patches(os.path.join(output_dir, f'layer_{i}_patches'), layer=i)\n",
    "\n",
    "        image_batch, offset_xs, offset_ys = image_preprocessor.get_image_patches(i)\n",
    "        spotter.load_batch(image_batch, offset_xs, offset_ys)\n",
    "        results = spotter.inference_batch()\n",
    "        all_layer_results.extend(results)\n",
    "\n",
    "        #all_layer_offset_xs.extend(offset_xs)\n",
    "        #all_layer_offset_ys.extend(offset_ys)\n",
    "\n",
    "        if save_each_layer == True:\n",
    "            vis.draw(results).save(os.path.join(output_dir, f'combined_tagged_{i}.png'))\n",
    "            vis.save_json(results, os.path.join(output_dir, f'combined_tagged_{i}.json'))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    vis.draw(all_layer_results).save(os.path.join(output_dir, f'combined_tagged_all_layers.png'))\n",
    "    vis.save_json(all_layer_results, os.path.join(output_dir, f'combined_tagged_all_layers.json'))\n",
    "\n",
    "# Run crop on all maps\n",
    "for map_data_subfolder in next(os.walk(map_data_topfolder))[1]:\n",
    "    jpeg_list = glob.glob(map_data_topfolder + '/' + map_data_subfolder + '/*.jpeg')\n",
    "    if len(jpeg_list) != 1:\n",
    "        print(map_data_subfolder + \" failed. Please ensure there is exactly 1 file with extension .jpeg in the folder.\")\n",
    "    else:\n",
    "        map_image = jpeg_list[0].split(\"\\\\\")[1]\n",
    "        if map_data_subfolder in ['1846_vandevelde', '1874_saunders', '1845_kiepert']: # '1858_vandevelde', '1874_saunders', '1845_kiepert']: #,,]: #'1858_vandevelde', '1847_tobler', '1845_kiepert'\n",
    "            img_path = map_data_topfolder + '/' + map_data_subfolder + \"/\" + map_image\n",
    "            map_name = os.path.basename(img_path).split('.')[0] # get the map name without extension\n",
    "            output_dir = os.path.join(map_strec_topfolder, map_name)\n",
    "            if not os.path.isdir(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            pyramid_scan(img_path, output_dir, save_each_layer=False)\n",
    "            logging.info('Done cropping %s' %img_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Label Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import json \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "import Clustering\n",
    "import TextRectify\n",
    "import TextAmalgamate\n",
    "import ExtractHandling\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(TextRectify)\n",
    "importlib.reload(TextAmalgamate)\n",
    "importlib.reload(ExtractHandling)\n",
    "\n",
    "map_name_in_strec = 'kiepert_1845'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Text Rectification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_cluster_pre_merge = True\n",
    "\n",
    "with open(f'processed/strec/{map_name_in_strec}/combined_tagged_all_layers.json', 'r', encoding='utf-8') as f:\n",
    "\n",
    "    clustered = Clustering.cluster_polygons(json.load(f))\n",
    "\n",
    "    # visualize clusters\n",
    "    #image = Clustering.visualize_polygons(clustered, 'processed/strec/kiepert_1845/raw.jpeg')\n",
    "    #image.save('processed/strec/kiepert_1845/combined_tagged_all_layers_clustering.png')\n",
    "\n",
    "for label, cluster in clustered.items():\n",
    "    texts = []\n",
    "    scores = []\n",
    "    for polygon in cluster:\n",
    "        texts.append(polygon['text'])\n",
    "        scores.append(polygon['score'])\n",
    "\n",
    "    rectifier = TextRectify.TextRectifier(0.95, 0.5, 10, True, True)\n",
    "\n",
    "    rectifier.feed_data(texts, scores)\n",
    "\n",
    "    rectifier.fit()\n",
    "\n",
    "    rectified, mask = rectifier.get_rectified_text()\n",
    "\n",
    "    if rectified is None:\n",
    "        rectified = max(texts, key=len)\n",
    "\n",
    "    for i in range(len(cluster)):\n",
    "        cluster[i]['text'] = rectified[i]\n",
    "        cluster[i]['keep'] = mask[i]\n",
    "\n",
    "image = Clustering.visualize_polygons(clustered, f'processed/strec/{map_name_in_strec}/raw.jpeg')\n",
    "image.save(f'processed/strec/{map_name_in_strec}/combined_tagged_all_layers_rectified.png')\n",
    "\n",
    "polygon_x = {}\n",
    "polygon_y = {}\n",
    "texts = {}\n",
    "scores = {}\n",
    "i = 0\n",
    "for label, cluster in clustered.items():\n",
    "    for polygon in cluster:\n",
    "        if do_cluster_pre_merge:\n",
    "            if polygon['keep']:\n",
    "                polygon_x[str(i)] = polygon['polygon_x']\n",
    "                polygon_y[str(i)] = polygon['polygon_y']\n",
    "                texts[str(i)] = polygon['text']\n",
    "                scores[str(i)] = polygon['score']\n",
    "                i += 1\n",
    "        else:\n",
    "            polygon_x[str(i)] = polygon['polygon_x']\n",
    "            polygon_y[str(i)] = polygon['polygon_y']\n",
    "            texts[str(i)] = polygon['text']\n",
    "            scores[str(i)] = polygon['score']\n",
    "            i += 1\n",
    "\n",
    "json_data = {'polygon_x': polygon_x, 'polygon_y': polygon_y, 'text': texts, 'score': scores}\n",
    "\n",
    "with open(f'processed/strec/{map_name_in_strec}/combined_tagged_all_layers_rectified_premerge.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Text Amalgamation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1507 labels.\n",
      "1429 labels.\n",
      "1403 labels.\n",
      "1400 labels.\n",
      "1397 labels.\n",
      "1395 labels.\n",
      "Amalgamation completed with 1395 labels.\n"
     ]
    }
   ],
   "source": [
    "map_name_in_strec = \"kiepert_1845\"\n",
    "\n",
    "# Amalgamation stage - assumes there exists \"combined_tagged_all_layers_rectified_premerge.json\" in map_name_in_strec processed folder.\n",
    "df = ExtractHandling.prepare_labels_for_amalgamation(map_name_in_strec)\n",
    "df = TextAmalgamate.amalgamate_labels_wrapper(df, 0.75, .5)\n",
    "\n",
    "# Save amalgamated labels\n",
    "with open(f'processed/strec/{map_name_in_strec}/processed_extracted_labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list(df[\"labels\"])\n",
    "polygons = []\n",
    "texts = []\n",
    "PCA_features = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    poly = result[i][0]\n",
    "    polygons.append(poly)\n",
    "    texts.append(result[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload SpotterWrapper module\n",
    "import importlib\n",
    "import SpotterWrapper\n",
    "import Grouping\n",
    "\n",
    "importlib.reload(SpotterWrapper)\n",
    "importlib.reload(Grouping)\n",
    "\n",
    "PCA_features = Grouping.calc_PCA_feats(polygons, do_separation=True, enhance_coords=True)\n",
    "\n",
    "print(\"PCA features calculated.\")\n",
    "\n",
    "vis = SpotterWrapper.PolygonVisualizer()\n",
    "canvas = Image.open(f'processed/strec/{map_name_in_strec}/raw.jpeg')\n",
    "vis.canvas_from_image(canvas)\n",
    "\n",
    "vis.draw_poly(polygons, texts, PCA_features)\n",
    "\n",
    "vis.save(f'processed/strec/{map_name_in_strec}/output.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import scipy\n",
    "import numpy as np\n",
    "import importlib \n",
    "\n",
    "import Evaluation\n",
    "importlib.reload(Evaluation)\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: Isolate crops to be used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_crop(map_name_in_strec, raw_or_spotter, left_x, right_x, top_y, bottom_y):\n",
    "    if raw_or_spotter == \"raw\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/raw.jpeg') \n",
    "    elif raw_or_spotter == \"spotter_0\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_0.png')\n",
    "    elif raw_or_spotter == \"spotter_1\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_1.png')\n",
    "    elif raw_or_spotter == \"spotter_2\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_2.png')\n",
    "    elif raw_or_spotter == \"all\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_all_layers.png')\n",
    "    elif raw_or_spotter == \"rectified\":\n",
    "        map_img = Image.open('processed/strec/' + map_name_in_strec + '/combined_tagged_all_layers_rectified.png')\n",
    "    width, height = map_img.size\n",
    "    print(\"full map is \" + str(width) + \" pixels wide by \" + str(height) + \" pixels high.\\n displaying crop:\")\n",
    "    display(map_img.crop((left_x, top_y, right_x, bottom_y, )))\n",
    "\n",
    "left_x = 2475\n",
    "right_x = 3550\n",
    "top_y = 4820\n",
    "bottom_y = 5850\n",
    "\n",
    "#visualize_crop(\"kiepert_1845\", \"all\", left_x, right_x, top_y, bottom_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiepert_gt_patch_1 = [2475, 3550, 4820, 5850]\n",
    "saunders_gt_patch_1 = [3150, 4150, 2250, 3250]\n",
    "saunders_gt_patch_2 = [6750, 7750, 2250, 3250]\n",
    "saunders_gt_patch_3 = [5400, 6400, 4500, 5500]\n",
    "saunders_gt_patch_4 = [7650, 8650, 5400, 6400]\n",
    "saunders_gt_patch_5 = [7650, 8650, 3150, 4150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Precision and Recall: IoU after 1:1 Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retaining 49 labels fully inside crop area\n",
      "retaining 43 labels that have alphabetic characters\n",
      "retaining 74 labels fully inside crop area\n",
      "retaining 69 labels that have alphabetic characters\n",
      "retaining 6 labels fully inside crop area\n",
      "retaining 6 labels that have alphabetic characters\n",
      "retaining 13 labels fully inside crop area\n",
      "retaining 11 labels that have alphabetic characters\n",
      "retaining 11 labels fully inside crop area\n",
      "retaining 10 labels that have alphabetic characters\n",
      "retaining 35 labels fully inside crop area\n",
      "retaining 32 labels that have alphabetic characters\n",
      "retaining 47 labels fully inside crop area\n",
      "retaining 46 labels that have alphabetic characters\n",
      "retaining 87 labels fully inside crop area\n",
      "retaining 87 labels that have alphabetic characters\n",
      "retaining 25 labels fully inside crop area\n",
      "retaining 23 labels that have alphabetic characters\n",
      "retaining 65 labels fully inside crop area\n",
      "retaining 60 labels that have alphabetic characters\n",
      "retaining 33 labels fully inside crop area\n",
      "retaining 33 labels that have alphabetic characters\n",
      "retaining 71 labels fully inside crop area\n",
      "retaining 67 labels that have alphabetic characters\n",
      "retaining 49 labels fully inside crop area\n",
      "retaining 43 labels that have alphabetic characters\n",
      "retaining 32 labels fully inside crop area\n",
      "retaining 32 labels that have alphabetic characters\n",
      "retaining 6 labels fully inside crop area\n",
      "retaining 6 labels that have alphabetic characters\n",
      "retaining 7 labels fully inside crop area\n",
      "retaining 6 labels that have alphabetic characters\n",
      "retaining 11 labels fully inside crop area\n",
      "retaining 10 labels that have alphabetic characters\n",
      "retaining 7 labels fully inside crop area\n",
      "retaining 5 labels that have alphabetic characters\n",
      "retaining 47 labels fully inside crop area\n",
      "retaining 46 labels that have alphabetic characters\n",
      "retaining 9 labels fully inside crop area\n",
      "retaining 8 labels that have alphabetic characters\n",
      "retaining 25 labels fully inside crop area\n",
      "retaining 23 labels that have alphabetic characters\n",
      "retaining 0 labels fully inside crop area\n",
      "retaining 33 labels fully inside crop area\n",
      "retaining 33 labels that have alphabetic characters\n",
      "retaining 0 labels fully inside crop area\n"
     ]
    }
   ],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "## Patch-level geographic pairings for non-multiline-toponyms against our pyramid pipeline\n",
    "pyramid_detected_kiepert, num_gt_kiepert, pyramid_IoU_pairs_kiepert = Evaluation.geographic_evaluation(\"kiepert_1845\", \"components\", kiepert_gt_patch_1)\n",
    "pyramid_detected_saunders1, num_gt_saunders1, pyramid_IoU_pairs_saunders1 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", saunders_gt_patch_1)\n",
    "pyramid_detected_saunders2, num_gt_saunders2, pyramid_IoU_pairs_saunders2 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", saunders_gt_patch_2)\n",
    "pyramid_detected_saunders3, num_gt_saunders3, pyramid_IoU_pairs_saunders3 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", saunders_gt_patch_3)\n",
    "pyramid_detected_saunders4, num_gt_saunders4, pyramid_IoU_pairs_saunders4 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", saunders_gt_patch_4)\n",
    "pyramid_detected_saunders5, num_gt_saunders5, pyramid_IoU_pairs_saunders5 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", saunders_gt_patch_5)\n",
    "\n",
    "## Aggregating pyramid numbers and pairs to map-level figures\n",
    "num_gt_kiepert = num_gt_kiepert\n",
    "num_gt_saunders = num_gt_saunders1 + num_gt_saunders2 + num_gt_saunders3 + num_gt_saunders4 + num_gt_saunders5\n",
    "pyramid_detected_kiepert = pyramid_detected_kiepert\n",
    "pyramid_detected_saunders = pyramid_detected_saunders1 + pyramid_detected_saunders2 + pyramid_detected_saunders3 + pyramid_detected_saunders4 + pyramid_detected_saunders5\n",
    "pyramid_IoU_pairs_kiepert = pyramid_IoU_pairs_kiepert\n",
    "pyramid_IoU_pairs_saunders = np.concatenate((pyramid_IoU_pairs_saunders1, pyramid_IoU_pairs_saunders2, pyramid_IoU_pairs_saunders3, pyramid_IoU_pairs_saunders4, pyramid_IoU_pairs_saunders5))\n",
    "\n",
    "## Patch-level geographic pairings for non-multiline-toponyms against baseline\n",
    "baseline_detected_kiepert, num_gt_kiepert, baseline_IoU_pairs_kiepert = Evaluation.geographic_evaluation(\"kiepert_1845\", \"components\", [2475, 3550, 4820, 5850], \"combined_tagged_0.json\")\n",
    "baseline_detected_saunders1, num_gt_saunders1, baseline_IoU_pairs_saunders1 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", [3150, 4150, 2250, 3250], \"combined_tagged_0.json\")\n",
    "baseline_detected_saunders2, num_gt_saunders2, baseline_IoU_pairs_saunders2 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", [6750, 7750, 2250, 3250], \"combined_tagged_0.json\")\n",
    "baseline_detected_saunders3, num_gt_saunders3, baseline_IoU_pairs_saunders3 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", [5400, 6400, 4500, 5500], \"combined_tagged_0.json\")\n",
    "baseline_detected_saunders4, num_gt_saunders4, baseline_IoU_pairs_saunders4 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", [7650, 8650, 5400, 6400], \"combined_tagged_0.json\")\n",
    "baseline_detected_saunders5, num_gt_saunders5, baseline_IoU_pairs_saunders5 = Evaluation.geographic_evaluation(\"saunders_1874\", \"components\", [7650, 8650, 3150, 4150], \"combined_tagged_0.json\")\n",
    "\n",
    "## Aggregate baseline numbers and pairs to map-level figures\n",
    "baseline_detected_kiepert = baseline_detected_kiepert\n",
    "baseline_detected_saunders = baseline_detected_saunders1 + baseline_detected_saunders2 + baseline_detected_saunders3 + baseline_detected_saunders4 + baseline_detected_saunders5\n",
    "baseline_IoU_pairs_kiepert = baseline_IoU_pairs_kiepert\n",
    "baseline_IoU_pairs_saunders = np.concatenate((baseline_IoU_pairs_saunders1, baseline_IoU_pairs_saunders2, baseline_IoU_pairs_saunders3, baseline_IoU_pairs_saunders4, baseline_IoU_pairs_saunders5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kiepert baseline\n",
      "\n",
      "Avg of Geographic Precision: 0.19116442583187376\n",
      "Avg of Geographic Recall: 0.14226189829348745\n",
      "Avg of Text Precision: 0.08627091726132564\n",
      "Avg of Text Recall: 0.06420161284563769\n",
      "\n",
      "kiepert pyramid\n",
      "\n",
      "Avg of Geographic Precision: 0.31668587247270935\n",
      "Avg of Geographic Recall: 0.5081703535027197\n",
      "Avg of Text Precision: 0.2951939227107476\n",
      "Avg of Text Recall: 0.47368327132654847\n",
      "\n",
      "saunders baseline\n",
      "\n",
      "Avg of Geographic Precision: 0.06051490020519661\n",
      "Avg of Geographic Recall: 0.00974392460931132\n",
      "Avg of Text Precision: 0.08252460898511729\n",
      "Avg of Text Recall: 0.013287860768790072\n",
      "\n",
      "saunders pyramid\n",
      "\n",
      "Avg of Geographic Precision: 0.21679753988318085\n",
      "Avg of Geographic Recall: 0.4721776927964193\n",
      "Avg of Text Precision: 0.191391017192887\n",
      "Avg of Text Recall: 0.4168431476150166\n"
     ]
    }
   ],
   "source": [
    "# Gimme them numbers :)\n",
    "print(\"\\nkiepert baseline\\n\")\n",
    "Evaluation.prec_rec(baseline_IoU_pairs_kiepert, baseline_detected_kiepert, num_gt_kiepert)\n",
    "print(\"\\nkiepert pyramid\\n\")\n",
    "Evaluation.prec_rec(pyramid_IoU_pairs_kiepert, pyramid_detected_kiepert, num_gt_kiepert)\n",
    "print(\"\\nsaunders baseline\\n\")\n",
    "Evaluation.prec_rec(baseline_IoU_pairs_saunders, baseline_detected_saunders, num_gt_saunders)\n",
    "print(\"\\nsaunders pyramid\\n\")\n",
    "Evaluation.prec_rec(pyramid_IoU_pairs_saunders, pyramid_detected_saunders, num_gt_saunders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Word Sequence Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(POLYGON ((1804.4089355469 245.5939331055, 181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(POLYGON ((2981.6633300781 683.8120117188, 298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(POLYGON ((2653.9155273438 685.1826171875, 265...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(POLYGON ((3261.9147949219 1565.3248291016, 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(POLYGON ((1053.6634521484 539.9749145508, 105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>(POLYGON ((4973.0122070312 4527.1484375, 4978....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>(POLYGON ((2991.8229980469 2267.4406738281, 29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>(POLYGON ((2648.1437988281 1277.6254882812, 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>(POLYGON ((3270.8348527274193 2733.29252845014...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>(POLYGON ((2276.2324277036914 3088.56325904374...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1395 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 labels\n",
       "1     (POLYGON ((1804.4089355469 245.5939331055, 181...\n",
       "4     (POLYGON ((2981.6633300781 683.8120117188, 298...\n",
       "5     (POLYGON ((2653.9155273438 685.1826171875, 265...\n",
       "7     (POLYGON ((3261.9147949219 1565.3248291016, 32...\n",
       "8     (POLYGON ((1053.6634521484 539.9749145508, 105...\n",
       "...                                                 ...\n",
       "1609  (POLYGON ((4973.0122070312 4527.1484375, 4978....\n",
       "1610  (POLYGON ((2991.8229980469 2267.4406738281, 29...\n",
       "1615  (POLYGON ((2648.1437988281 1277.6254882812, 27...\n",
       "1617  (POLYGON ((3270.8348527274193 2733.29252845014...\n",
       "1618  (POLYGON ((2276.2324277036914 3088.56325904374...\n",
       "\n",
       "[1395 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "map_name_in_strec = \"kiepert_1845\"\n",
    "df = pickle.load(open('processed/strec/' + map_name_in_strec + '/processed_extracted_labels.pickle', 'rb'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list(df[\"labels\"])\n",
    "polygons = []\n",
    "texts = []\n",
    "PCA_features = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    poly = result[i][0]\n",
    "    polygons.append(poly)\n",
    "    texts.append(result[i][1])\n",
    "\n",
    "#reload SpotterWrapper module\n",
    "import importlib\n",
    "import SpotterWrapper\n",
    "import Grouping\n",
    "\n",
    "importlib.reload(SpotterWrapper)\n",
    "importlib.reload(Grouping)\n",
    "\n",
    "PCA_features = Grouping.calc_PCA_feats(polygons, do_separation=True, enhance_coords=True)\n",
    "\n",
    "print(\"PCA features calculated.\")\n",
    "\n",
    "vis = SpotterWrapper.PolygonVisualizer()\n",
    "canvas = Image.open(f'processed/strec/{map_name_in_strec}/raw.jpeg')\n",
    "vis.canvas_from_image(canvas)\n",
    "\n",
    "vis.draw_poly(polygons, texts, PCA_features)\n",
    "\n",
    "vis.save(f'processed/strec/{map_name_in_strec}/output.jpeg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
